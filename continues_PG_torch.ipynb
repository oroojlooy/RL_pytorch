{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "continues-PG-torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oroojlooy/RL_pytorch/blob/master/continues_PG_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJqIx--gMVhT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_nPm6rkFo3L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjTnUfcUA_Bh"
      },
      "source": [
        "# !pip install --upgrade gym\n",
        "# !pip install --upgrade torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmqNcyIXAT8Y"
      },
      "source": [
        "import argparse\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Y73Kd4AT8d"
      },
      "source": [
        "# parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwbldOLQAT8f"
      },
      "source": [
        "parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
        "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
        "                    help='discount factor (default: 0.99)')\n",
        "parser.add_argument('--entropy_alpha', type=float, default=0.001, metavar='G',\n",
        "                    help='entropy coefficient in the loss (default: 0.001)')\n",
        "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
        "                    help='random seed (default: 543)')\n",
        "parser.add_argument('--render', action='store_true',\n",
        "                    help='render the environment')\n",
        "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "                    help='interval between training status logs (default: 10)')\n",
        "parser.add_argument('--alg', type=str, default='reinforce', \n",
        "                    help='the algorithm to train the agent')\n",
        "args, unknown = parser.parse_known_args()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7tCuXp-AT8i"
      },
      "source": [
        "# create an environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcX1dN16AT8j",
        "outputId": "166fc794-9022-4f6d-ed91-0b02b2b50538"
      },
      "source": [
        "# this environment has env.reset() and end.step() functions\n",
        "env = gym.make('Pendulum-v0')\n",
        "env.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f559ff22730>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y254LZFAT8p",
        "outputId": "de02fd2c-d56b-4e87-8810-c7df1d28494b"
      },
      "source": [
        "env.reset()\n",
        "# env.render()\n",
        "env.step([1.])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.97927093, 0.20255479, 0.8098795 ]), -0.05664824675073252, False, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7jaeYIlAT8u"
      },
      "source": [
        "# create actor network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8K7leovBfIa"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnGQtBAEAT8v"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    # this class defines a policy network with two layer NN\n",
        "    def __init__(self):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1_mu = nn.Linear(3, 256)\n",
        "        self.fc2_mu = nn.Linear(256, 1)\n",
        "\n",
        "        self.fc1_sigma = nn.Linear(3, 256)\n",
        "        self.fc2_sigma = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' do the forward pass and return a probability over actions\n",
        "        Input:\n",
        "                x: state\n",
        "        returns:\n",
        "                prob: a probability distribution\n",
        "        '''\n",
        "        out = F.relu(self.fc1_mu(x))\n",
        "        mu = self.fc2_mu(out)\n",
        "        \n",
        "        out = F.relu(self.fc1_sigma(x))\n",
        "        out = self.fc2_sigma(out)\n",
        "        sigma = F.softplus(out) + 1e-5\n",
        "        return mu, sigma"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXQuyDXhAT8z"
      },
      "source": [
        "# Critic network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TlyZZTQAT8z"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    # this class defines a policy network with two layer NN\n",
        "    def __init__(self, in_d=3):\n",
        "        super(Critic, self).__init__()\n",
        "        self.affine1 = nn.Linear(in_d, 256)\n",
        "        self.affine2 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' do the forward pass and return a probability over actions\n",
        "        Input:\n",
        "                x: state\n",
        "        returns:\n",
        "                v: value of being at x\n",
        "        '''\n",
        "        \n",
        "        x = F.relu(self.affine1(x))\n",
        "        v = self.affine2(x).squeeze()\n",
        "        return v\n",
        "\n",
        "class QCritic(nn.Module):\n",
        "    # this class defines a policy network with two layer NN\n",
        "    def __init__(self, in_d=4):\n",
        "        super(Critic, self).__init__()\n",
        "        self.affine1 = nn.Linear(in_d, 30)\n",
        "        self.affine2 = nn.Linear(30, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' do the forward pass and return a probability over actions\n",
        "        Input:\n",
        "                x: state\n",
        "        returns:\n",
        "                v: value of being at x\n",
        "        '''\n",
        "        \n",
        "        x = F.relu(self.affine1(x))\n",
        "        v = self.affine2(x).squeeze()\n",
        "        return v\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-__a1C3aAT84"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az8jMJnnDoau"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATtRw2qSAT8-"
      },
      "source": [
        "# rollout funtion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBWDaywwAT9A"
      },
      "source": [
        "def select_action_manual(state, env):\n",
        "    # this function selects stochastic actions based on the policy probabilities\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    mu, sigma = actor(state)\n",
        "    action = mu + torch.rand(state.size(0))*sigma\n",
        "    action = torch.clip(action, env.action_space.low[0], env.action_space.high[0])\n",
        "    entropy = 0.5*torch.log(2*np.pi*sigma) + 0.5\n",
        "    log_prob = -((action - mu)**2/(2*(sigma)**2)) -torch.log(sigma) -np.log(2*np.pi)\n",
        "    \n",
        "    return action.item(), log_prob, entropy, mu, sigma\n",
        "\n",
        "\n",
        "def select_action(state, env):\n",
        "    # this function selects stochastic actions based on the policy probabilities\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    mu, sigma = actor(state)\n",
        "    dist = torch.distributions.normal.Normal(mu, sigma)\n",
        "    action = dist.sample()\n",
        "    action = torch.clip(action, env.action_space.low[0], env.action_space.high[0])\n",
        "    entropy = dist.entropy()\n",
        "    log_prob = dist.log_prob(action)\n",
        "    \n",
        "    return action.item(), log_prob, entropy, mu, sigma    \n",
        "\n",
        "# mu, sigma = torch.tensor([0.]), torch.tensor([1.])\n",
        "# dist = torch.distributions.normal.Normal(mu, sigma)\n",
        "# a = dist.sample()\n",
        "# print(a, dist.entropy(), 0.5*torch.log(2*np.pi*sigma) + 0.5, dist.log_prob(a), -((a - mu)**2/(2*(sigma)**2)) -torch.log(sigma) -0.5*np.log(2*np.pi))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfowBl_lBsUW"
      },
      "source": [
        "# s=env.reset()\n",
        "# actor = Actor()\n",
        "# select_action(s)\n",
        "# state = torch.from_numpy(s).float().unsqueeze(0)\n",
        "# probs = actor(state)\n",
        "# m = Categorical(probs)\n",
        "# action = m.sample()\n",
        "# log_prob = m.log_prob(action)\n",
        "# print(probs, m, action, log_prob)\n",
        "# # (tensor([[0.5007, 0.4993]], grad_fn=<SoftmaxBackward>), Categorical(), tensor([1]), tensor([-0.6946], grad_fn=<SqueezeBackward1>))\n",
        "# np.log(0.4993)\n",
        "# # -0.6945481614755734\n",
        "\n",
        "# states, rewards, log_probs, entropies, next_states, mask, mus, sigmas = rollout()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtYTcf_JgPiR"
      },
      "source": [
        "# %debug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny9KIlz9gPoA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_RYkLaKCPxR"
      },
      "source": [
        "# states[0]\n",
        "# rewards[0]\n",
        "# log_probs[0]\n",
        "# entropies[0]\n",
        "# next_states[0]\n",
        "# entropies\n",
        "# log_probs\n",
        "# sigmas\n",
        "# torch.log(torch.stack(sigmas))\n",
        "# sigmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ05O7LkuTjs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAzOnWWKAT9F"
      },
      "source": [
        "def rollout(env, render=False, pause=.2):\n",
        "    states = []\n",
        "    rewards = []\n",
        "    log_probs = []\n",
        "    entropies = []\n",
        "    next_states = []\n",
        "    mask = []\n",
        "    mus = []\n",
        "    sigmas = []\n",
        "    # play an episode\n",
        "    state = env.reset()\n",
        "    while True:  # Don't infinite loop while learning\n",
        "        # select an action\n",
        "        action, log_prob, entropy, mu, sigma = select_action(state, env)\n",
        "        states.append(list(state))\n",
        "        log_probs.append(log_prob)\n",
        "        entropies.append(entropy)\n",
        "        mus.append(mu)\n",
        "        sigmas.append(sigma)\n",
        "        \n",
        "        # take the action and move to next state\n",
        "        next_state, reward, done, _ = env.step([action])\n",
        "        rewards.append(reward)\n",
        "        next_states.append(next_state)\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(pause)\n",
        "        if done:\n",
        "            mask.append(1)\n",
        "            break\n",
        "        mask.append(0)\n",
        "        state = next_state\n",
        "            \n",
        "    return states, rewards, log_probs, entropies, next_states, mask, mus, sigmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSZPgcmcAT9I"
      },
      "source": [
        "# train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq_H6jcUAT9J"
      },
      "source": [
        "def reinforce_train_step(args, states, rewards, log_probs, entropies, critic):\n",
        "    \n",
        "    R = 0\n",
        "    P = 0\n",
        "    E = 0\n",
        "    rewards_path = []\n",
        "    log_probs_paths = []\n",
        "    entropies_path = []\n",
        "    for i in reversed(range(len(rewards))):\n",
        "        R = rewards[i] + args.gamma * R\n",
        "        rewards_path.insert(0, R) \n",
        "        \n",
        "        P = log_probs[i] + P\n",
        "        log_probs_paths.insert(0, P) \n",
        "\n",
        "        E = entropies[i] + args.gamma * E\n",
        "        entropies_path.insert(0, E)\n",
        "\n",
        "    rewards_path = torch.tensor(rewards_path, dtype=torch.float32)\n",
        "    rewards_path = (rewards_path - rewards_path.mean()) / (rewards_path.std() + 1e-8)\n",
        "    log_probs_paths = torch.stack(log_probs_paths)\n",
        "    \n",
        "#     print(rewards_path,log_probs_paths)\n",
        "    value = critic(torch.tensor(states, dtype=torch.float32))\n",
        "\n",
        "    # take a backward step for actor\n",
        "    # This is based on Pytorch implementation of REINFORCE \n",
        "    actor_loss = -torch.mean(((rewards_path - value.detach()) * torch.stack(log_probs)) -args.entropy_alpha * torch.stack(entropies_path))\n",
        "    # This is based on the formual which Levine obtains for REINFORCE algorithm\n",
        "    # actor_loss = -torch.mean(((rewards_path - value.detach()) * log_probs_paths))\n",
        "    actor_optim.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optim.step()\n",
        "\n",
        "    # take a backward step for critic\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    critic_loss = loss_fn(value, rewards_path)\n",
        "    critic_optim.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optim.step()\n",
        "\n",
        "    return actor_loss, critic_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfAh0rSMaDl4"
      },
      "source": [
        "# Actor Critic \n",
        "def ac_train_step(args, states, rewards, log_probs, entropies, next_states, done, critic):\n",
        "# states, rewards, log_probs, next_states = rollout()\n",
        "# if 1 == 1:\n",
        "\n",
        "    P = 0\n",
        "    E = 0\n",
        "    rewards_path = []\n",
        "    log_probs_paths = []\n",
        "    qvalue = critic(torch.tensor(states, dtype=torch.float32))\n",
        "    next_qvalue = critic(torch.tensor(next_states, dtype=torch.float32))\n",
        "    next_qvalue = next_qvalue.detach().cpu().numpy()\n",
        "    target = []\n",
        "    for i in range(len(rewards)):\n",
        "        target.append(rewards[i] + args.gamma * next_qvalue[i]*(1-done[i]))\n",
        "       \n",
        "    target = torch.tensor(target)\n",
        "\n",
        "    qvalue = (qvalue - torch.mean(qvalue))/(torch.std(qvalue) + 1e-8)\n",
        "    # take a backward step for actor\n",
        "    # This is based on the commonly used AC algorithm  \n",
        "    actor_loss = -torch.mean(qvalue.detach() * torch.stack(log_probs) - args.entropy_alpha * torch.stack(entropies))\n",
        "    actor_optim.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optim.step()\n",
        "\n",
        "    # take a backward step for critic\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    critic_loss = loss_fn(qvalue, torch.tensor(target, dtype=torch.float32))\n",
        "    critic_optim.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optim.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u69K431OFQf-"
      },
      "source": [
        "def rtg_reinforce_train_step(args, states, rewards_, log_probs, entropies, critic):\n",
        "    \n",
        "    R = 0\n",
        "    G = 0\n",
        "    E = 0\n",
        "    log_probs_grads = []\n",
        "    entropies_path = []\n",
        "    rewards = []\n",
        "#     print(rewards_path,log_probs_paths)\n",
        "    value = critic(torch.tensor(states, dtype=torch.float32))\n",
        "    for i in reversed(range(len(rewards_))):\n",
        "        R = rewards_[i] + args.gamma * R\n",
        "        rewards.insert(0, R)\n",
        "\n",
        "        E = entropies[i] + args.gamma * E\n",
        "        entropies_path.insert(0, E)\n",
        "        \n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
        "    value_detached = value.detach()\n",
        "    for i in reversed(range(len(rewards_))):\n",
        "        G += log_probs[i]*(rewards[i] - value_detached[i]) \n",
        "        log_probs_grads.insert(0, G) \n",
        "\n",
        "    log_probs_grads = torch.stack(log_probs_grads)\n",
        "    \n",
        "    # take a backward step for actor\n",
        "    # This is based on Pytorch implementation of REINFORCE \n",
        "    actor_loss = -torch.mean(log_probs_grads - args.entropy_alpha * torch.stack(entropies_path))\n",
        "    actor_optim.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optim.step()\n",
        "\n",
        "    # take a backward step for critic\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    critic_loss = loss_fn(value, rewards)\n",
        "    critic_optim.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optim.step()\n",
        "\n",
        "    return actor_loss, critic_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-asw4wcRh-G"
      },
      "source": [
        "# Advantage Actor Critic  (single worker A2C)\n",
        "def aac_train_step(args, states, rewards, log_probs, next_states, done, critic):\n",
        "# states, rewards, log_probs, next_states = rollout()\n",
        "# if 1 == 1:\n",
        "    P = 0\n",
        "    rewards_path = []\n",
        "    log_probs_paths = []\n",
        "    value = critic(torch.tensor(states, dtype=torch.float32))\n",
        "    next_value = critic(torch.tensor(next_states, dtype=torch.float32))\n",
        "    next_value = next_value.detach().cpu().numpy()\n",
        "    advantage = []\n",
        "    target = []\n",
        "    for i in range(len(rewards)):\n",
        "        target.append(rewards[i] + args.gamma * next_value[i]*(1-done[i]))\n",
        "        advantage.append(target[i] - value[i].detach().cpu().numpy())\n",
        "        \n",
        "        P = log_probs[i] + P\n",
        "        log_probs_paths.insert(0, P) \n",
        "\n",
        "    advantage = torch.tensor(advantage)\n",
        "    target = torch.tensor(target)\n",
        "    cumulative_log_probs = torch.stack(log_probs_paths)\n",
        "    \n",
        "#     print(log_probs_paths)\n",
        "\n",
        "    # take a backward step for actor\n",
        "    # This is based on Pytorch implementation of AC \n",
        "    actor_loss = -torch.mean(advantage.detach() * torch.squeeze(torch.stack(log_probs)) - args.entropy_alpha * torch.stack(entropies))\n",
        "    # This is based on the formual which Levine obtains for AC algorithm\n",
        "    # actor_loss = -torch.mean(advantage.detach() * cumulative_log_probs)\n",
        "    actor_optim.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optim.step()\n",
        "\n",
        "    # take a backward step for critic\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    critic_loss = loss_fn(value, torch.tensor(target, dtype=torch.float32))\n",
        "    critic_optim.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optim.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4zw7GSYAT9M"
      },
      "source": [
        "# run training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FGjCmdndDcj",
        "outputId": "bf5e5aa5-b745-4cf2-9ebc-53a0ddb37ec2"
      },
      "source": [
        "# create actor and critic network\n",
        "# args.alg = 'reinforcertg'\n",
        "args.alg= 'reinforce'\n",
        "if args.alg == 'ac_q':\n",
        "  critic = QCritic()\n",
        "else:\n",
        "  critic = Critic()\n",
        "\n",
        "args.entropy_alpha = 7e-4\n",
        "actor = Actor()\n",
        "\n",
        "# create optimizers\n",
        "actor_optim = optim.Adam(actor.parameters(), lr=1e-3)\n",
        "critic_optim = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "args.log_interval = 200\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print(args)\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pxqb4tH4AT9N",
        "scrolled": true,
        "outputId": "6c3e2323-669c-419e-ae26-c59e842ecb7c"
      },
      "source": [
        "running_reward = 0\n",
        "for i_episode in range(200000):\n",
        "    states, rewards, log_probs, entropies, next_states, done, mus, sigmas = rollout(env)\n",
        "    running_reward = 0.9*running_reward + 0.1*np.sum(rewards)\n",
        "    if args.alg == 'reinforce':\n",
        "        actor_loss, critic_loss = reinforce_train_step(args, states, rewards, log_probs, entropies, critic)\n",
        "    elif args.alg == 'ac':\n",
        "        actor_loss, critic_loss = ac_train_step(args, states, rewards, log_probs, entropies, next_states, done, critic)\n",
        "    elif args.alg == 'reinforcertg':\n",
        "        actor_loss, critic_loss = rtg_reinforce_train_step(args, states, rewards, log_probs, entropies, critic)\n",
        "    elif args.alg == 'aac':\n",
        "        actor_loss, critic_loss = aac_train_step(args, states, rewards, log_probs, next_states, done, critic)\n",
        "    if i_episode % args.log_interval == 0:\n",
        "        print('Episode={} \\tAverage reward: {:.2f} \\tActor-loss'\n",
        "        '={:.2f}, \\tCritic-loss={:.2f}, \\tentropy={:.2f}, \\tlog-prob={:.2f}, \\tmu={:.2f}, \\tsigma={:.4f}'.format(\n",
        "            i_episode, running_reward, actor_loss, critic_loss, torch.mean(\n",
        "                torch.stack(entropies)).item(), torch.mean(torch.stack(\n",
        "                    log_probs)).item(), torch.mean(torch.stack(mus)\n",
        "                    ).item(), torch.mean(torch.stack(sigmas)).item()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode=0 \tAverage reward: -148.62 \tActor-loss=43.92, \tCritic-loss=1.07, \tentropy=1.01, \tlog-prob=-1.20, \tmu=1.06, \tsigma=0.4537\n",
            "Episode=200 \tAverage reward: -1358.72 \tActor-loss=-55.87, \tCritic-loss=0.94, \tentropy=0.75, \tlog-prob=-0.67, \tmu=0.91, \tsigma=0.2995\n",
            "Episode=400 \tAverage reward: -1394.82 \tActor-loss=209.75, \tCritic-loss=1.13, \tentropy=1.35, \tlog-prob=-1.87, \tmu=0.29, \tsigma=0.8788\n",
            "Episode=600 \tAverage reward: -1655.09 \tActor-loss=91.59, \tCritic-loss=0.69, \tentropy=1.32, \tlog-prob=-1.79, \tmu=0.07, \tsigma=0.8956\n",
            "Episode=800 \tAverage reward: -1499.29 \tActor-loss=-44.09, \tCritic-loss=0.94, \tentropy=1.25, \tlog-prob=-1.67, \tmu=-0.48, \tsigma=0.7592\n",
            "Episode=1000 \tAverage reward: -1498.38 \tActor-loss=-1636.89, \tCritic-loss=0.98, \tentropy=1.95, \tlog-prob=-47.78, \tmu=-20.31, \tsigma=3.1392\n",
            "Episode=1200 \tAverage reward: -1511.05 \tActor-loss=-12794.98, \tCritic-loss=0.98, \tentropy=1.87, \tlog-prob=-2123.42, \tmu=-46.67, \tsigma=3.4451\n",
            "Episode=1400 \tAverage reward: -1243.74 \tActor-loss=26019614720.00, \tCritic-loss=0.97, \tentropy=-0.14, \tlog-prob=-248763520.00, \tmu=-47.63, \tsigma=0.1811\n",
            "Episode=1600 \tAverage reward: -1498.37 \tActor-loss=224.48, \tCritic-loss=0.96, \tentropy=2.72, \tlog-prob=-67.02, \tmu=-126.77, \tsigma=13.8569\n",
            "Episode=1800 \tAverage reward: -1343.80 \tActor-loss=-1281770846683136.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-151403620204544.00, \tmu=-176.58, \tsigma=0.0000\n",
            "Episode=2000 \tAverage reward: -1407.67 \tActor-loss=-518.86, \tCritic-loss=0.95, \tentropy=3.03, \tlog-prob=-36.65, \tmu=-184.91, \tsigma=25.5558\n",
            "Episode=2200 \tAverage reward: -1431.69 \tActor-loss=4427026944.00, \tCritic-loss=0.99, \tentropy=1.89, \tlog-prob=-102323888.00, \tmu=-183.89, \tsigma=6.2389\n",
            "Episode=2400 \tAverage reward: -1398.19 \tActor-loss=7201291763712.00, \tCritic-loss=1.07, \tentropy=0.85, \tlog-prob=-1657478184960.00, \tmu=-239.17, \tsigma=2.5241\n",
            "Episode=2600 \tAverage reward: -1328.44 \tActor-loss=-667.88, \tCritic-loss=0.95, \tentropy=3.13, \tlog-prob=-48.88, \tmu=-260.54, \tsigma=31.5433\n",
            "Episode=2800 \tAverage reward: -1536.80 \tActor-loss=-1177.87, \tCritic-loss=0.96, \tentropy=3.14, \tlog-prob=-50.14, \tmu=-272.82, \tsigma=32.2085\n",
            "Episode=3000 \tAverage reward: -1458.94 \tActor-loss=189.31, \tCritic-loss=0.94, \tentropy=3.09, \tlog-prob=-64.90, \tmu=-277.51, \tsigma=29.1622\n",
            "Episode=3200 \tAverage reward: -1396.31 \tActor-loss=240151317970944.00, \tCritic-loss=0.94, \tentropy=-0.25, \tlog-prob=-12779278827520.00, \tmu=-293.18, \tsigma=2.9516\n",
            "Episode=3400 \tAverage reward: -1369.23 \tActor-loss=1425385694691328.00, \tCritic-loss=1.01, \tentropy=-4.34, \tlog-prob=-714459152646144.00, \tmu=-380.76, \tsigma=0.0000\n",
            "Episode=3600 \tAverage reward: -1360.84 \tActor-loss=1970517573632.00, \tCritic-loss=0.90, \tentropy=0.76, \tlog-prob=-63927435264.00, \tmu=-246.97, \tsigma=2.3733\n",
            "Episode=3800 \tAverage reward: -1442.14 \tActor-loss=-95565.03, \tCritic-loss=0.97, \tentropy=2.65, \tlog-prob=-1273.14, \tmu=-335.37, \tsigma=13.6169\n",
            "Episode=4000 \tAverage reward: -1452.13 \tActor-loss=-11403242323836928.00, \tCritic-loss=1.00, \tentropy=-4.26, \tlog-prob=-860853716910080.00, \tmu=-451.03, \tsigma=0.0000\n",
            "Episode=4200 \tAverage reward: -1449.37 \tActor-loss=-135643497758720.00, \tCritic-loss=1.01, \tentropy=1.16, \tlog-prob=-2487164076032.00, \tmu=-392.22, \tsigma=5.4397\n",
            "Episode=4400 \tAverage reward: -1413.75 \tActor-loss=-2321.13, \tCritic-loss=0.94, \tentropy=2.95, \tlog-prob=-249.28, \tmu=-393.23, \tsigma=21.9798\n",
            "Episode=4600 \tAverage reward: -1469.42 \tActor-loss=-1717.44, \tCritic-loss=0.95, \tentropy=2.94, \tlog-prob=-258.48, \tmu=-388.02, \tsigma=21.4949\n",
            "Episode=4800 \tAverage reward: -1447.32 \tActor-loss=-2862709519417344.00, \tCritic-loss=0.99, \tentropy=-1.37, \tlog-prob=-436000585154560.00, \tmu=-430.68, \tsigma=2.5617\n",
            "Episode=5000 \tAverage reward: -1435.44 \tActor-loss=1899818519625728.00, \tCritic-loss=0.98, \tentropy=-1.52, \tlog-prob=-467591814447104.00, \tmu=-457.87, \tsigma=2.7807\n",
            "Episode=5200 \tAverage reward: -1478.72 \tActor-loss=-7441.62, \tCritic-loss=0.94, \tentropy=2.83, \tlog-prob=-835.03, \tmu=-487.35, \tsigma=18.0828\n",
            "Episode=5400 \tAverage reward: -1448.01 \tActor-loss=-1458210116468736.00, \tCritic-loss=1.00, \tentropy=0.13, \tlog-prob=-20004699373568.00, \tmu=-408.65, \tsigma=7.3784\n",
            "Episode=5600 \tAverage reward: -1445.75 \tActor-loss=891405094354944.00, \tCritic-loss=0.95, \tentropy=-0.03, \tlog-prob=-71791099772928.00, \tmu=-426.05, \tsigma=6.5515\n",
            "Episode=5800 \tAverage reward: -1430.66 \tActor-loss=-40825.21, \tCritic-loss=0.96, \tentropy=2.80, \tlog-prob=-1642.07, \tmu=-589.36, \tsigma=17.5009\n",
            "Episode=6000 \tAverage reward: -1416.04 \tActor-loss=21009.82, \tCritic-loss=0.89, \tentropy=2.78, \tlog-prob=-1760.74, \tmu=-576.35, \tsigma=16.8614\n",
            "Episode=6200 \tAverage reward: -1482.42 \tActor-loss=-8170019397566464.00, \tCritic-loss=0.99, \tentropy=-1.23, \tlog-prob=-663344478420992.00, \tmu=-528.08, \tsigma=3.0442\n",
            "Episode=6400 \tAverage reward: -1466.32 \tActor-loss=-226070609920.00, \tCritic-loss=0.89, \tentropy=2.51, \tlog-prob=-2662012416.00, \tmu=-702.37, \tsigma=15.7326\n",
            "Episode=6600 \tAverage reward: -1505.20 \tActor-loss=5173.05, \tCritic-loss=0.94, \tentropy=3.00, \tlog-prob=-961.29, \tmu=-784.36, \tsigma=24.7432\n",
            "Episode=6800 \tAverage reward: -1437.31 \tActor-loss=-59336.00, \tCritic-loss=0.92, \tentropy=2.85, \tlog-prob=-2414.74, \tmu=-737.88, \tsigma=19.3388\n",
            "Episode=7000 \tAverage reward: -1487.29 \tActor-loss=-85332.69, \tCritic-loss=0.95, \tentropy=2.83, \tlog-prob=-4055.36, \tmu=-859.08, \tsigma=19.1570\n",
            "Episode=7200 \tAverage reward: -1464.61 \tActor-loss=-46709.07, \tCritic-loss=0.94, \tentropy=2.84, \tlog-prob=-3574.29, \tmu=-886.37, \tsigma=19.3359\n",
            "Episode=7400 \tAverage reward: -1390.29 \tActor-loss=-67273.73, \tCritic-loss=0.91, \tentropy=2.99, \tlog-prob=-1854.46, \tmu=-989.70, \tsigma=25.2335\n",
            "Episode=7600 \tAverage reward: -1432.79 \tActor-loss=-2127788807028736.00, \tCritic-loss=0.98, \tentropy=-1.23, \tlog-prob=-1198683630600192.00, \tmu=-631.51, \tsigma=5.2084\n",
            "Episode=7800 \tAverage reward: -1342.61 \tActor-loss=-22799471083520.00, \tCritic-loss=0.95, \tentropy=1.04, \tlog-prob=-5369284788224.00, \tmu=-504.08, \tsigma=10.6967\n",
            "Episode=8000 \tAverage reward: -1467.69 \tActor-loss=-37657.20, \tCritic-loss=0.95, \tentropy=3.02, \tlog-prob=-1863.05, \tmu=-1100.17, \tsigma=26.5622\n",
            "Episode=8200 \tAverage reward: -1404.23 \tActor-loss=-2421686976643072.00, \tCritic-loss=0.96, \tentropy=-0.46, \tlog-prob=-309026386608128.00, \tmu=-588.88, \tsigma=4.4753\n",
            "Episode=8400 \tAverage reward: -1462.55 \tActor-loss=-14272364229427200.00, \tCritic-loss=1.01, \tentropy=-0.39, \tlog-prob=-356929230602240.00, \tmu=-597.69, \tsigma=3.5188\n",
            "Episode=8600 \tAverage reward: -1482.21 \tActor-loss=-16492.95, \tCritic-loss=0.94, \tentropy=3.12, \tlog-prob=-1324.15, \tmu=-1242.23, \tsigma=31.6660\n",
            "Episode=8800 \tAverage reward: -1382.14 \tActor-loss=-61887154430672896.00, \tCritic-loss=1.00, \tentropy=-4.27, \tlog-prob=-2778412196298752.00, \tmu=-766.05, \tsigma=0.0000\n",
            "Episode=9000 \tAverage reward: -1448.73 \tActor-loss=1782.18, \tCritic-loss=0.94, \tentropy=3.16, \tlog-prob=-1495.75, \tmu=-1435.05, \tsigma=34.1498\n",
            "Episode=9200 \tAverage reward: -1483.79 \tActor-loss=20780267253792768.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-3452513218985984.00, \tmu=-834.77, \tsigma=0.0000\n",
            "Episode=9400 \tAverage reward: -1394.63 \tActor-loss=-10933.33, \tCritic-loss=0.94, \tentropy=3.08, \tlog-prob=-3191.44, \tmu=-1350.30, \tsigma=31.1207\n",
            "Episode=9600 \tAverage reward: -1404.41 \tActor-loss=-47903522598944768.00, \tCritic-loss=0.99, \tentropy=-3.78, \tlog-prob=-3143088742596608.00, \tmu=-855.20, \tsigma=0.0002\n",
            "Episode=9800 \tAverage reward: -1445.37 \tActor-loss=-54633.73, \tCritic-loss=0.94, \tentropy=3.11, \tlog-prob=-2582.05, \tmu=-1646.04, \tsigma=31.3627\n",
            "Episode=10000 \tAverage reward: -1291.25 \tActor-loss=-8767358617255936.00, \tCritic-loss=0.99, \tentropy=-1.29, \tlog-prob=-1634197743599616.00, \tmu=-806.76, \tsigma=1.8679\n",
            "Episode=10200 \tAverage reward: -1504.46 \tActor-loss=146362.66, \tCritic-loss=1.00, \tentropy=3.16, \tlog-prob=-2380.52, \tmu=-1833.94, \tsigma=34.2609\n",
            "Episode=10400 \tAverage reward: -1475.69 \tActor-loss=-76698.48, \tCritic-loss=0.95, \tentropy=3.16, \tlog-prob=-2729.40, \tmu=-1939.53, \tsigma=33.9233\n",
            "Episode=10600 \tAverage reward: -1412.12 \tActor-loss=-108502.74, \tCritic-loss=1.02, \tentropy=3.24, \tlog-prob=-1796.31, \tmu=-1926.71, \tsigma=39.2817\n",
            "Episode=10800 \tAverage reward: -1487.01 \tActor-loss=-12438446480031744.00, \tCritic-loss=1.00, \tentropy=-4.18, \tlog-prob=-3764485684723712.00, \tmu=-947.68, \tsigma=0.0000\n",
            "Episode=11000 \tAverage reward: -1484.82 \tActor-loss=-227516.19, \tCritic-loss=0.89, \tentropy=3.07, \tlog-prob=-5741.78, \tmu=-1688.76, \tsigma=31.4404\n",
            "Episode=11200 \tAverage reward: -1457.17 \tActor-loss=-28657.24, \tCritic-loss=0.91, \tentropy=3.12, \tlog-prob=-4271.94, \tmu=-2114.62, \tsigma=32.0420\n",
            "Episode=11400 \tAverage reward: -1497.38 \tActor-loss=32972592780410880.00, \tCritic-loss=0.99, \tentropy=-1.33, \tlog-prob=-4259237731500032.00, \tmu=-923.07, \tsigma=3.7702\n",
            "Episode=11600 \tAverage reward: -1498.16 \tActor-loss=-31838.92, \tCritic-loss=0.94, \tentropy=3.26, \tlog-prob=-2358.49, \tmu=-2306.64, \tsigma=41.1833\n",
            "Episode=11800 \tAverage reward: -1469.09 \tActor-loss=2610099138854912.00, \tCritic-loss=1.00, \tentropy=0.49, \tlog-prob=-111067594227712.00, \tmu=-936.45, \tsigma=8.8129\n",
            "Episode=12000 \tAverage reward: -1471.61 \tActor-loss=-24423889874452480.00, \tCritic-loss=0.99, \tentropy=-0.76, \tlog-prob=-2322555910225920.00, \tmu=-1029.76, \tsigma=6.7086\n",
            "Episode=12200 \tAverage reward: -1468.03 \tActor-loss=-91449.11, \tCritic-loss=0.88, \tentropy=3.25, \tlog-prob=-3295.93, \tmu=-2594.73, \tsigma=41.2415\n",
            "Episode=12400 \tAverage reward: -1483.28 \tActor-loss=126041991464091648.00, \tCritic-loss=0.98, \tentropy=-2.90, \tlog-prob=-6308923631992832.00, \tmu=-1139.94, \tsigma=0.0758\n",
            "Episode=12600 \tAverage reward: -1430.84 \tActor-loss=-398457.31, \tCritic-loss=0.88, \tentropy=3.20, \tlog-prob=-5633.01, \tmu=-2627.91, \tsigma=40.2562\n",
            "Episode=12800 \tAverage reward: -1395.31 \tActor-loss=-75892.89, \tCritic-loss=0.89, \tentropy=3.17, \tlog-prob=-5074.69, \tmu=-2758.32, \tsigma=36.5741\n",
            "Episode=13000 \tAverage reward: -1533.16 \tActor-loss=-57073.77, \tCritic-loss=0.94, \tentropy=3.30, \tlog-prob=-3618.07, \tmu=-3131.27, \tsigma=45.3026\n",
            "Episode=13200 \tAverage reward: -1535.99 \tActor-loss=26541.82, \tCritic-loss=0.94, \tentropy=3.37, \tlog-prob=-2675.23, \tmu=-3192.79, \tsigma=50.7240\n",
            "Episode=13400 \tAverage reward: -1477.17 \tActor-loss=-783297210744832.00, \tCritic-loss=0.98, \tentropy=0.95, \tlog-prob=-73896690712576.00, \tmu=-1095.08, \tsigma=9.7794\n",
            "Episode=13600 \tAverage reward: -1417.36 \tActor-loss=-27811839534432256.00, \tCritic-loss=1.00, \tentropy=-0.19, \tlog-prob=-879677820370944.00, \tmu=-1124.97, \tsigma=3.8714\n",
            "Episode=13800 \tAverage reward: -1500.78 \tActor-loss=-126681872871718912.00, \tCritic-loss=1.01, \tentropy=-1.32, \tlog-prob=-7114034578980864.00, \tmu=-1171.58, \tsigma=3.4633\n",
            "Episode=14000 \tAverage reward: -1487.58 \tActor-loss=41081.94, \tCritic-loss=0.90, \tentropy=3.36, \tlog-prob=-3568.03, \tmu=-3533.04, \tsigma=50.4034\n",
            "Episode=14200 \tAverage reward: -1513.33 \tActor-loss=-15006189456719872.00, \tCritic-loss=0.99, \tentropy=-0.97, \tlog-prob=-2742759135903744.00, \tmu=-1228.66, \tsigma=3.8290\n",
            "Episode=14400 \tAverage reward: -1455.38 \tActor-loss=74835.76, \tCritic-loss=0.93, \tentropy=3.33, \tlog-prob=-4846.27, \tmu=-3797.42, \tsigma=47.8965\n",
            "Episode=14600 \tAverage reward: -1466.90 \tActor-loss=31574.87, \tCritic-loss=0.93, \tentropy=3.41, \tlog-prob=-3283.37, \tmu=-3893.76, \tsigma=55.3975\n",
            "Episode=14800 \tAverage reward: -1528.63 \tActor-loss=51804869027168256.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-11988821386199040.00, \tmu=-1540.64, \tsigma=0.0000\n",
            "Episode=15000 \tAverage reward: -1507.21 \tActor-loss=-108077.06, \tCritic-loss=0.88, \tentropy=3.40, \tlog-prob=-3350.77, \tmu=-3859.37, \tsigma=56.1143\n",
            "Episode=15200 \tAverage reward: -1460.33 \tActor-loss=42859736344821760.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-12746504083079168.00, \tmu=-1576.70, \tsigma=0.0000\n",
            "Episode=15400 \tAverage reward: -1543.87 \tActor-loss=-10777.48, \tCritic-loss=0.88, \tentropy=3.34, \tlog-prob=-5620.45, \tmu=-4111.89, \tsigma=49.4619\n",
            "Episode=15600 \tAverage reward: -1494.63 \tActor-loss=-79130889778364416.00, \tCritic-loss=0.99, \tentropy=-0.50, \tlog-prob=-3316375103733760.00, \tmu=-1384.25, \tsigma=7.3498\n",
            "Episode=15800 \tAverage reward: -1467.93 \tActor-loss=-99283.13, \tCritic-loss=0.95, \tentropy=3.42, \tlog-prob=-4059.58, \tmu=-4398.43, \tsigma=56.6977\n",
            "Episode=16000 \tAverage reward: -1477.48 \tActor-loss=-327313891350544384.00, \tCritic-loss=1.00, \tentropy=-4.30, \tlog-prob=-15729582208450560.00, \tmu=-1742.57, \tsigma=0.0000\n",
            "Episode=16200 \tAverage reward: -1509.25 \tActor-loss=183631824956686336.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-16330923800788992.00, \tmu=-1777.59, \tsigma=0.0000\n",
            "Episode=16400 \tAverage reward: -1508.95 \tActor-loss=64549.95, \tCritic-loss=0.94, \tentropy=3.45, \tlog-prob=-4333.76, \tmu=-4821.19, \tsigma=59.9266\n",
            "Episode=16600 \tAverage reward: -1544.28 \tActor-loss=-89688.05, \tCritic-loss=0.86, \tentropy=3.39, \tlog-prob=-5459.64, \tmu=-4560.03, \tsigma=56.9494\n",
            "Episode=16800 \tAverage reward: -1534.55 \tActor-loss=109962114944204800.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-18308242992005120.00, \tmu=-1887.66, \tsigma=0.0000\n",
            "Episode=17000 \tAverage reward: -1464.75 \tActor-loss=-544.79, \tCritic-loss=0.91, \tentropy=3.52, \tlog-prob=-10.37, \tmu=17.48, \tsigma=69.5347\n",
            "Episode=17200 \tAverage reward: -1523.47 \tActor-loss=3473489302388736.00, \tCritic-loss=1.00, \tentropy=-0.51, \tlog-prob=-1020780380946432.00, \tmu=-1832.17, \tsigma=3.5680\n",
            "Episode=17400 \tAverage reward: -1567.02 \tActor-loss=-362983616.00, \tCritic-loss=0.89, \tentropy=3.36, \tlog-prob=-2841111.75, \tmu=-342.70, \tsigma=64.3250\n",
            "Episode=17600 \tAverage reward: -1582.05 \tActor-loss=36.69, \tCritic-loss=0.93, \tentropy=3.53, \tlog-prob=-6.11, \tmu=18.62, \tsigma=70.7764\n",
            "Episode=17800 \tAverage reward: -1487.19 \tActor-loss=217085.27, \tCritic-loss=0.92, \tentropy=3.41, \tlog-prob=-4946.96, \tmu=-5079.40, \tsigma=58.3079\n",
            "Episode=18000 \tAverage reward: -1496.95 \tActor-loss=-129567283440779264.00, \tCritic-loss=0.98, \tentropy=-2.64, \tlog-prob=-28064993496793088.00, \tmu=-2021.16, \tsigma=0.5457\n",
            "Episode=18200 \tAverage reward: -1490.30 \tActor-loss=-240901.27, \tCritic-loss=0.86, \tentropy=3.34, \tlog-prob=-10555.52, \tmu=-5353.91, \tsigma=54.1553\n",
            "Episode=18400 \tAverage reward: -1490.15 \tActor-loss=-108371568295411712.00, \tCritic-loss=0.99, \tentropy=-1.16, \tlog-prob=-5499164993519616.00, \tmu=-1880.18, \tsigma=1.4236\n",
            "Episode=18600 \tAverage reward: -1506.80 \tActor-loss=-10968.67, \tCritic-loss=0.91, \tentropy=3.47, \tlog-prob=-5572.19, \tmu=-5884.39, \tsigma=63.2794\n",
            "Episode=18800 \tAverage reward: -1493.61 \tActor-loss=13212.54, \tCritic-loss=0.93, \tentropy=3.49, \tlog-prob=-6053.69, \tmu=-6175.28, \tsigma=64.6465\n",
            "Episode=19000 \tAverage reward: -1415.56 \tActor-loss=5373886988288.00, \tCritic-loss=0.96, \tentropy=1.38, \tlog-prob=-329618685952.00, \tmu=-2024.86, \tsigma=3.8902\n",
            "Episode=19200 \tAverage reward: -1475.38 \tActor-loss=-145587.75, \tCritic-loss=0.95, \tentropy=3.49, \tlog-prob=-6673.33, \tmu=-6547.49, \tsigma=64.7797\n",
            "Episode=19400 \tAverage reward: -1457.00 \tActor-loss=1460343641473024.00, \tCritic-loss=0.95, \tentropy=-0.93, \tlog-prob=-135856207691776.00, \tmu=-2169.60, \tsigma=0.0755\n",
            "Episode=19600 \tAverage reward: -1531.16 \tActor-loss=30980.98, \tCritic-loss=0.94, \tentropy=3.47, \tlog-prob=-8621.34, \tmu=-6946.26, \tsigma=62.8063\n",
            "Episode=19800 \tAverage reward: -1510.79 \tActor-loss=281265433846218752.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-35900991677136896.00, \tmu=-2666.79, \tsigma=0.0000\n",
            "Episode=20000 \tAverage reward: -1479.78 \tActor-loss=-175546102345367552.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-37651985419206656.00, \tmu=-2727.37, \tsigma=0.0000\n",
            "Episode=20200 \tAverage reward: -1453.24 \tActor-loss=90584906131832832.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-39106935655497728.00, \tmu=-2770.91, \tsigma=0.0000\n",
            "Episode=20400 \tAverage reward: -1486.60 \tActor-loss=85171.99, \tCritic-loss=0.89, \tentropy=3.55, \tlog-prob=-6982.66, \tmu=-7639.92, \tsigma=72.3033\n",
            "Episode=20600 \tAverage reward: -1433.02 \tActor-loss=-58305868965347328.00, \tCritic-loss=0.99, \tentropy=-1.32, \tlog-prob=-6495421849403392.00, \tmu=-2655.50, \tsigma=0.2770\n",
            "Episode=20800 \tAverage reward: -1449.08 \tActor-loss=-98547.27, \tCritic-loss=0.94, \tentropy=3.51, \tlog-prob=-9011.45, \tmu=-7900.29, \tsigma=67.1623\n",
            "Episode=21000 \tAverage reward: -1463.15 \tActor-loss=-361537.91, \tCritic-loss=0.86, \tentropy=3.23, \tlog-prob=-5565.43, \tmu=-5.10, \tsigma=44.5099\n",
            "Episode=21200 \tAverage reward: -1466.92 \tActor-loss=-41525891301376000.00, \tCritic-loss=1.00, \tentropy=-3.75, \tlog-prob=-24666226961678336.00, \tmu=-2564.92, \tsigma=0.0001\n",
            "Episode=21400 \tAverage reward: -1467.78 \tActor-loss=-223367883126734848.00, \tCritic-loss=0.99, \tentropy=-3.90, \tlog-prob=-30950812087746560.00, \tmu=-2865.63, \tsigma=0.0000\n",
            "Episode=21600 \tAverage reward: -1513.03 \tActor-loss=98806.81, \tCritic-loss=0.94, \tentropy=3.53, \tlog-prob=-8847.01, \tmu=-8294.76, \tsigma=70.8712\n",
            "Episode=21800 \tAverage reward: -1466.26 \tActor-loss=-40091617037647872.00, \tCritic-loss=0.99, \tentropy=-2.76, \tlog-prob=-16456679369474048.00, \tmu=-2651.27, \tsigma=0.0040\n",
            "Episode=22000 \tAverage reward: -1506.76 \tActor-loss=-630446.00, \tCritic-loss=0.85, \tentropy=3.42, \tlog-prob=-16528.38, \tmu=-7945.17, \tsigma=60.8324\n",
            "Episode=22200 \tAverage reward: -1513.18 \tActor-loss=-121846.36, \tCritic-loss=0.86, \tentropy=3.56, \tlog-prob=-8002.16, \tmu=-8416.56, \tsigma=75.8181\n",
            "Episode=22400 \tAverage reward: -1466.03 \tActor-loss=97044399506063360.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-55407238486949888.00, \tmu=-3080.04, \tsigma=0.0000\n",
            "Episode=22600 \tAverage reward: -1476.14 \tActor-loss=-25779.58, \tCritic-loss=0.94, \tentropy=3.56, \tlog-prob=-8911.12, \tmu=-8840.35, \tsigma=73.4890\n",
            "Episode=22800 \tAverage reward: -1488.94 \tActor-loss=-466077.47, \tCritic-loss=0.96, \tentropy=3.52, \tlog-prob=-11475.59, \tmu=-9086.86, \tsigma=68.2883\n",
            "Episode=23000 \tAverage reward: -1485.58 \tActor-loss=279633.78, \tCritic-loss=0.89, \tentropy=3.56, \tlog-prob=-9266.91, \tmu=-9012.34, \tsigma=74.8484\n",
            "Episode=23200 \tAverage reward: -1552.56 \tActor-loss=-175141.45, \tCritic-loss=0.93, \tentropy=3.58, \tlog-prob=-9044.75, \tmu=-9337.62, \tsigma=76.6414\n",
            "Episode=23400 \tAverage reward: -1498.48 \tActor-loss=237789.73, \tCritic-loss=0.94, \tentropy=3.57, \tlog-prob=-9656.65, \tmu=-9511.35, \tsigma=75.9410\n",
            "Episode=23600 \tAverage reward: -1504.35 \tActor-loss=-384343.09, \tCritic-loss=0.85, \tentropy=3.42, \tlog-prob=-18838.38, \tmu=-8813.65, \tsigma=62.7652\n",
            "Episode=23800 \tAverage reward: -1521.22 \tActor-loss=741035240432074752.00, \tCritic-loss=0.99, \tentropy=-4.28, \tlog-prob=-74776943991455744.00, \tmu=-3280.98, \tsigma=0.0000\n",
            "Episode=24000 \tAverage reward: -1541.49 \tActor-loss=125004.64, \tCritic-loss=0.87, \tentropy=3.55, \tlog-prob=-12255.53, \tmu=-9941.49, \tsigma=72.8349\n",
            "Episode=24200 \tAverage reward: -1499.32 \tActor-loss=-477565.16, \tCritic-loss=0.87, \tentropy=3.50, \tlog-prob=-15773.72, \tmu=-10053.45, \tsigma=68.0558\n",
            "Episode=24400 \tAverage reward: -1491.56 \tActor-loss=-491382.12, \tCritic-loss=0.88, \tentropy=3.38, \tlog-prob=-34268.78, \tmu=-10609.84, \tsigma=54.6785\n",
            "Episode=24600 \tAverage reward: -1462.77 \tActor-loss=-464947767618306048.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-78222272267026432.00, \tmu=-3822.16, \tsigma=0.0000\n",
            "Episode=24800 \tAverage reward: -1542.89 \tActor-loss=-5016407452518711296.00, \tCritic-loss=0.91, \tentropy=0.98, \tlog-prob=-173594194328158208.00, \tmu=-10706.14, \tsigma=20.5327\n",
            "Episode=25000 \tAverage reward: -1535.74 \tActor-loss=1189578809960038400.00, \tCritic-loss=0.98, \tentropy=-4.34, \tlog-prob=-82754356117504000.00, \tmu=-3737.42, \tsigma=0.0000\n",
            "Episode=25200 \tAverage reward: -1534.53 \tActor-loss=-36804369328373760.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-75313668874567680.00, \tmu=-3845.25, \tsigma=0.0000\n",
            "Episode=25400 \tAverage reward: -1519.36 \tActor-loss=-2866912144916480.00, \tCritic-loss=0.84, \tentropy=3.62, \tlog-prob=-37748626948096.00, \tmu=-11.86, \tsigma=103.6325\n",
            "Episode=25600 \tAverage reward: -1586.38 \tActor-loss=1167070157672022016.00, \tCritic-loss=0.93, \tentropy=0.34, \tlog-prob=-286674910795792384.00, \tmu=-10892.69, \tsigma=16.0156\n",
            "Episode=25800 \tAverage reward: -1554.51 \tActor-loss=-5679780716347392.00, \tCritic-loss=0.92, \tentropy=3.67, \tlog-prob=-65034705174528.00, \tmu=-12.65, \tsigma=117.0644\n",
            "Episode=26000 \tAverage reward: -1486.10 \tActor-loss=1912495748033806336.00, \tCritic-loss=0.98, \tentropy=-2.21, \tlog-prob=-105000018968051712.00, \tmu=-3408.75, \tsigma=2.6432\n",
            "Episode=26200 \tAverage reward: -1541.62 \tActor-loss=-6811227693711884288.00, \tCritic-loss=0.92, \tentropy=0.13, \tlog-prob=-307771755794006016.00, \tmu=-11100.34, \tsigma=14.8646\n",
            "Episode=26400 \tAverage reward: -1520.25 \tActor-loss=-22815471866740736.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-86212122618363904.00, \tmu=-3704.57, \tsigma=0.0000\n",
            "Episode=26600 \tAverage reward: -1562.56 \tActor-loss=1705833878939238400.00, \tCritic-loss=0.93, \tentropy=0.17, \tlog-prob=-306754226501976064.00, \tmu=-11189.62, \tsigma=15.4171\n",
            "Episode=26800 \tAverage reward: -1511.05 \tActor-loss=5611997508122181632.00, \tCritic-loss=1.02, \tentropy=-2.07, \tlog-prob=-111824215195254784.00, \tmu=-3455.01, \tsigma=3.3375\n",
            "Episode=27000 \tAverage reward: -1541.21 \tActor-loss=939026910585815040.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-85876694262480896.00, \tmu=-3769.17, \tsigma=0.0000\n",
            "Episode=27200 \tAverage reward: -1528.93 \tActor-loss=-770474320668393472.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-82410707194216448.00, \tmu=-3982.23, \tsigma=0.0000\n",
            "Episode=27400 \tAverage reward: -1557.41 \tActor-loss=-151.99, \tCritic-loss=0.94, \tentropy=3.76, \tlog-prob=-8.51, \tmu=13.36, \tsigma=115.0709\n",
            "Episode=27600 \tAverage reward: -1512.65 \tActor-loss=-1040316533076656128.00, \tCritic-loss=0.99, \tentropy=-3.14, \tlog-prob=-99248207355183104.00, \tmu=-3420.69, \tsigma=0.1530\n",
            "Episode=27800 \tAverage reward: -1581.31 \tActor-loss=-437627304017920.00, \tCritic-loss=0.87, \tentropy=3.73, \tlog-prob=-6639742091264.00, \tmu=6.25, \tsigma=121.8000\n",
            "Episode=28000 \tAverage reward: -1567.99 \tActor-loss=504881033506717696.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-82937124155817984.00, \tmu=-3997.28, \tsigma=0.0000\n",
            "Episode=28200 \tAverage reward: -1538.03 \tActor-loss=-4322883896229232640.00, \tCritic-loss=0.92, \tentropy=0.00, \tlog-prob=-338293442666823680.00, \tmu=-11110.25, \tsigma=13.8455\n",
            "Episode=28400 \tAverage reward: -1518.16 \tActor-loss=-133533677148700672.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-83362051040215040.00, \tmu=-4035.71, \tsigma=0.0000\n",
            "Episode=28600 \tAverage reward: -1556.21 \tActor-loss=4437692701378347008.00, \tCritic-loss=0.86, \tentropy=-0.27, \tlog-prob=-357404329147105280.00, \tmu=-11330.47, \tsigma=13.3675\n",
            "Episode=28800 \tAverage reward: -1542.89 \tActor-loss=-11559296739311616.00, \tCritic-loss=0.90, \tentropy=3.98, \tlog-prob=-239155657310208.00, \tmu=-13.46, \tsigma=220.8836\n",
            "Episode=29000 \tAverage reward: -1552.41 \tActor-loss=452731265720778752.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-83399039298568192.00, \tmu=-4045.62, \tsigma=0.0000\n",
            "Episode=29200 \tAverage reward: -1543.59 \tActor-loss=7973088777608364032.00, \tCritic-loss=0.84, \tentropy=-0.19, \tlog-prob=-358780058711621632.00, \tmu=-9802.45, \tsigma=20.9025\n",
            "Episode=29400 \tAverage reward: -1533.18 \tActor-loss=382187767913775104.00, \tCritic-loss=0.88, \tentropy=-0.84, \tlog-prob=-421378622935269376.00, \tmu=-11433.32, \tsigma=9.3991\n",
            "Episode=29600 \tAverage reward: -1572.79 \tActor-loss=-910028390914850816.00, \tCritic-loss=1.00, \tentropy=-3.22, \tlog-prob=-83520466613960704.00, \tmu=-4027.61, \tsigma=0.2568\n",
            "Episode=29800 \tAverage reward: -1536.39 \tActor-loss=-1386721657137135616.00, \tCritic-loss=0.89, \tentropy=0.29, \tlog-prob=-319450252908167168.00, \tmu=-10885.09, \tsigma=15.9136\n",
            "Episode=30000 \tAverage reward: -1546.03 \tActor-loss=695.81, \tCritic-loss=1.15, \tentropy=4.13, \tlog-prob=-7.42, \tmu=23.87, \tsigma=238.7967\n",
            "Episode=30200 \tAverage reward: -1566.37 \tActor-loss=137.19, \tCritic-loss=0.94, \tentropy=4.12, \tlog-prob=-7.26, \tmu=24.64, \tsigma=228.0745\n",
            "Episode=30400 \tAverage reward: -1595.83 \tActor-loss=4350261510275072.00, \tCritic-loss=0.93, \tentropy=2.22, \tlog-prob=-440844066750464.00, \tmu=-11446.07, \tsigma=30.5911\n",
            "Episode=30600 \tAverage reward: -1546.03 \tActor-loss=1929084354839969792.00, \tCritic-loss=1.01, \tentropy=-0.85, \tlog-prob=-106163156241350656.00, \tmu=-3483.79, \tsigma=20.5910\n",
            "Episode=30800 \tAverage reward: -1571.94 \tActor-loss=-21361706.00, \tCritic-loss=0.92, \tentropy=4.05, \tlog-prob=-768814.06, \tmu=5.33, \tsigma=209.0052\n",
            "Episode=31000 \tAverage reward: -1546.24 \tActor-loss=-686255372113018880.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-87117364285407232.00, \tmu=-4083.12, \tsigma=0.0000\n",
            "Episode=31200 \tAverage reward: -1540.78 \tActor-loss=-1341300282037895168.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-88290757940609024.00, \tmu=-4107.80, \tsigma=0.0000\n",
            "Episode=31400 \tAverage reward: -1550.54 \tActor-loss=-3456624314437074944.00, \tCritic-loss=0.90, \tentropy=1.24, \tlog-prob=-211107400764096512.00, \tmu=-11150.38, \tsigma=25.4493\n",
            "Episode=31600 \tAverage reward: -1588.90 \tActor-loss=-251422231547084800.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-85570067957284864.00, \tmu=-4112.94, \tsigma=0.0000\n",
            "Episode=31800 \tAverage reward: -1555.99 \tActor-loss=2871211063582392320.00, \tCritic-loss=0.86, \tentropy=0.50, \tlog-prob=-295843119784525824.00, \tmu=-11545.04, \tsigma=19.9115\n",
            "Episode=32000 \tAverage reward: -1546.83 \tActor-loss=-2285704491678302208.00, \tCritic-loss=0.93, \tentropy=0.96, \tlog-prob=-249042579146932224.00, \tmu=-11567.39, \tsigma=23.1719\n",
            "Episode=32200 \tAverage reward: -1569.43 \tActor-loss=535411104155697152.00, \tCritic-loss=0.86, \tentropy=0.69, \tlog-prob=-278909403766194176.00, \tmu=-11643.81, \tsigma=20.9059\n",
            "Episode=32400 \tAverage reward: -1529.25 \tActor-loss=-1493899026955763712.00, \tCritic-loss=1.00, \tentropy=-2.94, \tlog-prob=-92247067065319424.00, \tmu=-4110.09, \tsigma=0.8785\n",
            "Episode=32600 \tAverage reward: -1540.92 \tActor-loss=-6796159986364841984.00, \tCritic-loss=0.93, \tentropy=0.40, \tlog-prob=-334105952632438784.00, \tmu=-11541.75, \tsigma=17.8929\n",
            "Episode=32800 \tAverage reward: -1551.78 \tActor-loss=1154789437423484928.00, \tCritic-loss=0.93, \tentropy=1.73, \tlog-prob=-105923771944140800.00, \tmu=-11747.79, \tsigma=28.3207\n",
            "Episode=33000 \tAverage reward: -1555.39 \tActor-loss=282172874536517632.00, \tCritic-loss=0.86, \tentropy=2.25, \tlog-prob=-25915800451809280.00, \tmu=-10945.50, \tsigma=30.3306\n",
            "Episode=33200 \tAverage reward: -1583.28 \tActor-loss=4.26, \tCritic-loss=0.90, \tentropy=4.06, \tlog-prob=-9.50, \tmu=26.83, \tsigma=202.8533\n",
            "Episode=33400 \tAverage reward: -1559.96 \tActor-loss=1549057264713728000.00, \tCritic-loss=0.98, \tentropy=-3.43, \tlog-prob=-90454416435445760.00, \tmu=-4065.19, \tsigma=0.0498\n",
            "Episode=33600 \tAverage reward: -1519.77 \tActor-loss=4216067990858235904.00, \tCritic-loss=0.99, \tentropy=-1.63, \tlog-prob=-129213532754411520.00, \tmu=-4046.49, \tsigma=12.0643\n",
            "Episode=33800 \tAverage reward: -1552.63 \tActor-loss=6115472678573834240.00, \tCritic-loss=0.93, \tentropy=1.09, \tlog-prob=-241265149288120320.00, \tmu=-11679.36, \tsigma=24.1620\n",
            "Episode=34000 \tAverage reward: -1579.92 \tActor-loss=-7387.84, \tCritic-loss=0.85, \tentropy=4.03, \tlog-prob=-122.22, \tmu=18.44, \tsigma=195.7487\n",
            "Episode=34200 \tAverage reward: -1540.89 \tActor-loss=-211835449260376064.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-89381988871438336.00, \tmu=-4213.20, \tsigma=0.0000\n",
            "Episode=34400 \tAverage reward: -1501.69 \tActor-loss=3815570880440827904.00, \tCritic-loss=0.93, \tentropy=1.23, \tlog-prob=-215018123105927168.00, \tmu=-11705.84, \tsigma=24.7351\n",
            "Episode=34600 \tAverage reward: -1589.38 \tActor-loss=-2110153166652702720.00, \tCritic-loss=1.00, \tentropy=-3.84, \tlog-prob=-92029389532823552.00, \tmu=-4147.39, \tsigma=0.0011\n",
            "Episode=34800 \tAverage reward: -1540.20 \tActor-loss=855.21, \tCritic-loss=0.89, \tentropy=4.01, \tlog-prob=-61.64, \tmu=21.19, \tsigma=199.5569\n",
            "Episode=35000 \tAverage reward: -1542.33 \tActor-loss=923232907089149952.00, \tCritic-loss=0.98, \tentropy=-1.89, \tlog-prob=-124585224456634368.00, \tmu=-4137.39, \tsigma=9.0285\n",
            "Episode=35200 \tAverage reward: -1570.23 \tActor-loss=4013717444295131136.00, \tCritic-loss=0.93, \tentropy=1.32, \tlog-prob=-211450139154317312.00, \tmu=-11842.82, \tsigma=25.8545\n",
            "Episode=35400 \tAverage reward: -1533.91 \tActor-loss=23252117861462704128.00, \tCritic-loss=0.92, \tentropy=-0.31, \tlog-prob=-187628979220381696.00, \tmu=-5867.88, \tsigma=20.8050\n",
            "Episode=35600 \tAverage reward: -1561.43 \tActor-loss=-41676189990912.00, \tCritic-loss=0.92, \tentropy=3.96, \tlog-prob=-8055429267456.00, \tmu=12.21, \tsigma=195.9398\n",
            "Episode=35800 \tAverage reward: -1568.05 \tActor-loss=-727919234943811584.00, \tCritic-loss=0.99, \tentropy=-2.32, \tlog-prob=-103914775221633024.00, \tmu=-3821.07, \tsigma=2.9071\n",
            "Episode=36000 \tAverage reward: -1557.21 \tActor-loss=-837575316288307200.00, \tCritic-loss=0.99, \tentropy=-4.33, \tlog-prob=-105129735570325504.00, \tmu=-4259.43, \tsigma=0.0000\n",
            "Episode=36200 \tAverage reward: -1554.29 \tActor-loss=409888485906841600.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-97684727590289408.00, \tmu=-4213.42, \tsigma=0.0000\n",
            "Episode=36400 \tAverage reward: -1577.83 \tActor-loss=142892622413824.00, \tCritic-loss=0.90, \tentropy=3.79, \tlog-prob=-32937569419264.00, \tmu=-17.43, \tsigma=174.7001\n",
            "Episode=36600 \tAverage reward: -1565.36 \tActor-loss=13895869844391198720.00, \tCritic-loss=0.86, \tentropy=-0.93, \tlog-prob=-475747548783443968.00, \tmu=-11914.81, \tsigma=8.9799\n",
            "Episode=36800 \tAverage reward: -1570.54 \tActor-loss=3257684452943331328.00, \tCritic-loss=0.85, \tentropy=-0.90, \tlog-prob=-475428552972435456.00, \tmu=-12116.11, \tsigma=9.2635\n",
            "Episode=37000 \tAverage reward: -1521.32 \tActor-loss=-724644958035771392.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-95001421002309632.00, \tmu=-4268.25, \tsigma=0.0000\n",
            "Episode=37200 \tAverage reward: -1555.59 \tActor-loss=8614255789206405120.00, \tCritic-loss=0.85, \tentropy=-1.37, \tlog-prob=-521290351198076928.00, \tmu=-12069.42, \tsigma=6.2158\n",
            "Episode=37400 \tAverage reward: -1593.67 \tActor-loss=14.17, \tCritic-loss=0.92, \tentropy=4.11, \tlog-prob=-7.25, \tmu=50.97, \tsigma=225.5154\n",
            "Episode=37600 \tAverage reward: -1527.38 \tActor-loss=7767293735993344000.00, \tCritic-loss=0.88, \tentropy=-1.15, \tlog-prob=-480274238154997760.00, \tmu=-10224.43, \tsigma=3.5078\n",
            "Episode=37800 \tAverage reward: -1536.50 \tActor-loss=7903609538337570816.00, \tCritic-loss=0.93, \tentropy=-1.55, \tlog-prob=-558842006181249024.00, \tmu=-12209.79, \tsigma=5.0508\n",
            "Episode=38000 \tAverage reward: -1539.72 \tActor-loss=-5565140720592879616.00, \tCritic-loss=0.94, \tentropy=-1.75, \tlog-prob=-562746749928603648.00, \tmu=-12246.62, \tsigma=4.2081\n",
            "Episode=38200 \tAverage reward: -1556.98 \tActor-loss=18000922894971961344.00, \tCritic-loss=0.89, \tentropy=-1.83, \tlog-prob=-538915866188709888.00, \tmu=-11798.97, \tsigma=3.8285\n",
            "Episode=38400 \tAverage reward: -1571.63 \tActor-loss=16793087381627469824.00, \tCritic-loss=0.85, \tentropy=-1.61, \tlog-prob=-492803722909319168.00, \tmu=-11146.64, \tsigma=4.9119\n",
            "Episode=38600 \tAverage reward: -1497.99 \tActor-loss=-1773608875186978816.00, \tCritic-loss=0.73, \tentropy=2.99, \tlog-prob=-26138994869796864.00, \tmu=-741.23, \tsigma=174.2630\n",
            "Episode=38800 \tAverage reward: -1522.08 \tActor-loss=3259657251681468416.00, \tCritic-loss=0.98, \tentropy=-1.64, \tlog-prob=-133453644728107008.00, \tmu=-4195.74, \tsigma=15.3747\n",
            "Episode=39000 \tAverage reward: -1527.32 \tActor-loss=-238.06, \tCritic-loss=0.93, \tentropy=4.19, \tlog-prob=-11.53, \tmu=42.03, \tsigma=271.5295\n",
            "Episode=39200 \tAverage reward: -1532.79 \tActor-loss=-6933277332685717504.00, \tCritic-loss=0.92, \tentropy=-1.46, \tlog-prob=-532211697117560832.00, \tmu=-11958.40, \tsigma=5.1013\n",
            "Episode=39400 \tAverage reward: -1520.87 \tActor-loss=-494815382511550464.00, \tCritic-loss=0.99, \tentropy=-2.04, \tlog-prob=-97588726481289216.00, \tmu=-4025.02, \tsigma=6.1596\n",
            "Episode=39600 \tAverage reward: -1546.80 \tActor-loss=-900389659668905984.00, \tCritic-loss=0.99, \tentropy=-2.20, \tlog-prob=-105798144150732800.00, \tmu=-4272.21, \tsigma=6.4449\n",
            "Episode=39800 \tAverage reward: -1542.03 \tActor-loss=263197485684490240.00, \tCritic-loss=0.85, \tentropy=-1.60, \tlog-prob=-542743025646829568.00, \tmu=-12117.86, \tsigma=5.3201\n",
            "Episode=40000 \tAverage reward: -1610.26 \tActor-loss=9591866761834135552.00, \tCritic-loss=0.86, \tentropy=-1.94, \tlog-prob=-572927540407042048.00, \tmu=-12137.70, \tsigma=3.5511\n",
            "Episode=40200 \tAverage reward: -1500.88 \tActor-loss=-237894269975658496.00, \tCritic-loss=0.99, \tentropy=-2.23, \tlog-prob=-97920255006867456.00, \tmu=-4041.20, \tsigma=4.6670\n",
            "Episode=40400 \tAverage reward: -1525.36 \tActor-loss=5620358194539790336.00, \tCritic-loss=0.85, \tentropy=-1.81, \tlog-prob=-561193621034893312.00, \tmu=-12091.00, \tsigma=4.1664\n",
            "Episode=40600 \tAverage reward: -1566.75 \tActor-loss=-167.03, \tCritic-loss=0.95, \tentropy=4.21, \tlog-prob=-7.44, \tmu=40.81, \tsigma=274.7155\n",
            "Episode=40800 \tAverage reward: -1564.53 \tActor-loss=-635719450099712.00, \tCritic-loss=0.88, \tentropy=4.11, \tlog-prob=-162055541751808.00, \tmu=11.19, \tsigma=275.7539\n",
            "Episode=41000 \tAverage reward: -1532.53 \tActor-loss=216570788143169536.00, \tCritic-loss=0.99, \tentropy=-1.76, \tlog-prob=-123107695577268224.00, \tmu=-4211.06, \tsigma=13.7242\n",
            "Episode=41200 \tAverage reward: -1563.21 \tActor-loss=4286548060588212224.00, \tCritic-loss=0.93, \tentropy=-1.66, \tlog-prob=-564286512884088832.00, \tmu=-12154.07, \tsigma=4.9946\n",
            "Episode=41400 \tAverage reward: -1556.81 \tActor-loss=444287156224.00, \tCritic-loss=0.90, \tentropy=4.07, \tlog-prob=-15963783168.00, \tmu=-10.06, \tsigma=291.2438\n",
            "Episode=41600 \tAverage reward: -1557.97 \tActor-loss=375158521357926400.00, \tCritic-loss=1.00, \tentropy=-0.98, \tlog-prob=-106004242451398656.00, \tmu=-3821.21, \tsigma=23.5921\n",
            "Episode=41800 \tAverage reward: -1579.84 \tActor-loss=7755072664250613760.00, \tCritic-loss=0.84, \tentropy=-0.74, \tlog-prob=-457382199906271232.00, \tmu=-10459.06, \tsigma=22.0209\n",
            "Episode=42000 \tAverage reward: -1523.63 \tActor-loss=4158154239522111488.00, \tCritic-loss=0.93, \tentropy=-1.57, \tlog-prob=-556684626928599040.00, \tmu=-12177.83, \tsigma=5.5889\n",
            "Episode=42200 \tAverage reward: -1549.85 \tActor-loss=-9851861679384559616.00, \tCritic-loss=0.89, \tentropy=-1.43, \tlog-prob=-511830187512430592.00, \tmu=-11209.34, \tsigma=4.2883\n",
            "Episode=42400 \tAverage reward: -1588.99 \tActor-loss=11226990085918425088.00, \tCritic-loss=0.79, \tentropy=-1.09, \tlog-prob=-437737844128088064.00, \tmu=-9949.06, \tsigma=19.8492\n",
            "Episode=42600 \tAverage reward: -1586.44 \tActor-loss=-5642947660932448256.00, \tCritic-loss=0.88, \tentropy=-1.56, \tlog-prob=-525929809230954496.00, \tmu=-11471.08, \tsigma=4.1163\n",
            "Episode=42800 \tAverage reward: -1547.05 \tActor-loss=-18068053577405825024.00, \tCritic-loss=0.96, \tentropy=-1.64, \tlog-prob=-561205681303060480.00, \tmu=-12215.78, \tsigma=5.3538\n",
            "Episode=43000 \tAverage reward: -1586.97 \tActor-loss=6697123675516174336.00, \tCritic-loss=0.89, \tentropy=-1.73, \tlog-prob=-572681146723205120.00, \tmu=-12181.05, \tsigma=5.0678\n",
            "Episode=43200 \tAverage reward: -1557.07 \tActor-loss=169.06, \tCritic-loss=0.90, \tentropy=4.21, \tlog-prob=-7.44, \tmu=39.33, \tsigma=285.7805\n",
            "Episode=43400 \tAverage reward: -1535.08 \tActor-loss=-248.91, \tCritic-loss=0.91, \tentropy=4.20, \tlog-prob=-15.13, \tmu=24.67, \tsigma=290.5043\n",
            "Episode=43600 \tAverage reward: -1562.60 \tActor-loss=3648272214815408128.00, \tCritic-loss=0.85, \tentropy=-0.75, \tlog-prob=-438782208375783424.00, \tmu=-11529.25, \tsigma=10.9731\n",
            "Episode=43800 \tAverage reward: -1558.35 \tActor-loss=10344473674953785344.00, \tCritic-loss=0.93, \tentropy=-1.54, \tlog-prob=-566833703368523776.00, \tmu=-12175.66, \tsigma=5.7945\n",
            "Episode=44000 \tAverage reward: -1565.30 \tActor-loss=33.62, \tCritic-loss=0.91, \tentropy=4.22, \tlog-prob=-7.45, \tmu=30.62, \tsigma=285.2545\n",
            "Episode=44200 \tAverage reward: -1553.77 \tActor-loss=7404357592048205824.00, \tCritic-loss=0.85, \tentropy=-2.11, \tlog-prob=-592727958116892672.00, \tmu=-12160.52, \tsigma=3.0622\n",
            "Episode=44400 \tAverage reward: -1544.57 \tActor-loss=-12289294240210485248.00, \tCritic-loss=0.90, \tentropy=-1.89, \tlog-prob=-565360667024949248.00, \tmu=-11695.42, \tsigma=2.7640\n",
            "Episode=44600 \tAverage reward: -1532.75 \tActor-loss=415259187891666944.00, \tCritic-loss=0.99, \tentropy=-3.90, \tlog-prob=-97568497185325056.00, \tmu=-4295.27, \tsigma=0.0010\n",
            "Episode=44800 \tAverage reward: -1544.68 \tActor-loss=-986590958969683968.00, \tCritic-loss=0.83, \tentropy=3.11, \tlog-prob=-33781425316560896.00, \tmu=-820.47, \tsigma=249.7715\n",
            "Episode=45000 \tAverage reward: -1556.93 \tActor-loss=-6881062075238449152.00, \tCritic-loss=0.94, \tentropy=-2.52, \tlog-prob=-628466277907693568.00, \tmu=-12210.59, \tsigma=1.7047\n",
            "Episode=45200 \tAverage reward: -1566.90 \tActor-loss=-807514324787527680.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-97588855330308096.00, \tmu=-4350.36, \tsigma=0.0000\n",
            "Episode=45400 \tAverage reward: -1559.01 \tActor-loss=31.04, \tCritic-loss=0.93, \tentropy=4.28, \tlog-prob=-7.57, \tmu=26.41, \tsigma=316.0172\n",
            "Episode=45600 \tAverage reward: -1541.59 \tActor-loss=-237741489399005184.00, \tCritic-loss=0.99, \tentropy=-1.97, \tlog-prob=-98875464323432448.00, \tmu=-4110.86, \tsigma=7.4057\n",
            "Episode=45800 \tAverage reward: -1545.83 \tActor-loss=-638982556871557120.00, \tCritic-loss=1.00, \tentropy=-2.70, \tlog-prob=-101683032905220096.00, \tmu=-4337.58, \tsigma=2.7125\n",
            "Episode=46000 \tAverage reward: -1563.55 \tActor-loss=-213.44, \tCritic-loss=0.95, \tentropy=4.30, \tlog-prob=-7.59, \tmu=24.77, \tsigma=324.2425\n",
            "Episode=46200 \tAverage reward: -1537.70 \tActor-loss=-405011499002626048.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-99933168739549184.00, \tmu=-4385.59, \tsigma=0.0000\n",
            "Episode=46400 \tAverage reward: -1526.28 \tActor-loss=1687798727147782144.00, \tCritic-loss=0.87, \tentropy=-2.77, \tlog-prob=-632207022623817728.00, \tmu=-12110.79, \tsigma=1.0374\n",
            "Episode=46600 \tAverage reward: -1539.52 \tActor-loss=2943990487003955200.00, \tCritic-loss=0.88, \tentropy=-3.65, \tlog-prob=-679745782240247808.00, \tmu=-12132.29, \tsigma=0.1629\n",
            "Episode=46800 \tAverage reward: -1555.71 \tActor-loss=-8080022115975168.00, \tCritic-loss=0.83, \tentropy=4.20, \tlog-prob=-129653528330240.00, \tmu=-14.43, \tsigma=307.5099\n",
            "Episode=47000 \tAverage reward: -1570.99 \tActor-loss=16159225525284372480.00, \tCritic-loss=0.88, \tentropy=-3.97, \tlog-prob=-700742949637455872.00, \tmu=-12111.07, \tsigma=0.1757\n",
            "Episode=47200 \tAverage reward: -1527.61 \tActor-loss=-5472043348787200.00, \tCritic-loss=0.82, \tentropy=4.15, \tlog-prob=-266611000868864.00, \tmu=-23.49, \tsigma=299.0629\n",
            "Episode=47400 \tAverage reward: -1524.77 \tActor-loss=-443976060787228672.00, \tCritic-loss=0.99, \tentropy=-1.04, \tlog-prob=-110589051320401920.00, \tmu=-3791.35, \tsigma=23.8522\n",
            "Episode=47600 \tAverage reward: -1566.09 \tActor-loss=10375663521298907136.00, \tCritic-loss=0.93, \tentropy=-3.47, \tlog-prob=-689222060643188736.00, \tmu=-12158.27, \tsigma=0.6940\n",
            "Episode=47800 \tAverage reward: -1548.46 \tActor-loss=11719851670627155968.00, \tCritic-loss=0.86, \tentropy=-3.20, \tlog-prob=-625037038579613696.00, \tmu=-11607.00, \tsigma=0.2902\n",
            "Episode=48000 \tAverage reward: -1569.18 \tActor-loss=-26093279237898240.00, \tCritic-loss=0.82, \tentropy=4.15, \tlog-prob=-567706227048448.00, \tmu=-42.91, \tsigma=323.4939\n",
            "Episode=48200 \tAverage reward: -1561.47 \tActor-loss=18106657430657040384.00, \tCritic-loss=0.86, \tentropy=-3.51, \tlog-prob=-658615024180264960.00, \tmu=-11852.19, \tsigma=0.0761\n",
            "Episode=48400 \tAverage reward: -1564.67 \tActor-loss=4282018897315495936.00, \tCritic-loss=0.84, \tentropy=-3.47, \tlog-prob=-675655324107014144.00, \tmu=-12030.02, \tsigma=0.2419\n",
            "Episode=48600 \tAverage reward: -1549.57 \tActor-loss=3184268962045100032.00, \tCritic-loss=0.88, \tentropy=-2.49, \tlog-prob=-609647930239877120.00, \tmu=-11245.01, \tsigma=1.0246\n",
            "Episode=48800 \tAverage reward: -1562.94 \tActor-loss=-213.49, \tCritic-loss=0.92, \tentropy=4.27, \tlog-prob=-11.87, \tmu=16.87, \tsigma=325.4347\n",
            "Episode=49000 \tAverage reward: -1576.15 \tActor-loss=-1333146991001075712.00, \tCritic-loss=0.93, \tentropy=-2.98, \tlog-prob=-667675824626860032.00, \tmu=-12235.49, \tsigma=0.9347\n",
            "Episode=49200 \tAverage reward: -1544.38 \tActor-loss=6326360658048712704.00, \tCritic-loss=0.93, \tentropy=-3.69, \tlog-prob=-710231185229348864.00, \tmu=-12241.79, \tsigma=0.3968\n",
            "Episode=49400 \tAverage reward: -1544.91 \tActor-loss=1316917099863474176.00, \tCritic-loss=0.98, \tentropy=-1.60, \tlog-prob=-130314539030806528.00, \tmu=-4267.46, \tsigma=20.6944\n",
            "Episode=49600 \tAverage reward: -1515.37 \tActor-loss=-401820338301698048.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-100934531774676992.00, \tmu=-4383.48, \tsigma=0.0000\n",
            "Episode=49800 \tAverage reward: -1512.77 \tActor-loss=-217306395781890048.00, \tCritic-loss=0.99, \tentropy=-0.95, \tlog-prob=-108353812900610048.00, \tmu=-3844.54, \tsigma=25.8383\n",
            "Episode=50000 \tAverage reward: -1562.42 \tActor-loss=-2355026363347369984.00, \tCritic-loss=0.93, \tentropy=-2.61, \tlog-prob=-648133929588490240.00, \tmu=-12232.44, \tsigma=1.8942\n",
            "Episode=50200 \tAverage reward: -1543.88 \tActor-loss=-4089988371185664.00, \tCritic-loss=0.84, \tentropy=4.23, \tlog-prob=-168850146459648.00, \tmu=-29.50, \tsigma=332.5276\n",
            "Episode=50400 \tAverage reward: -1546.16 \tActor-loss=-357551869863657472.00, \tCritic-loss=0.99, \tentropy=-1.92, \tlog-prob=-114798196579893248.00, \tmu=-4325.53, \tsigma=11.3721\n",
            "Episode=50600 \tAverage reward: -1590.33 \tActor-loss=-3574884486585778176.00, \tCritic-loss=0.93, \tentropy=-2.55, \tlog-prob=-655938263122444288.00, \tmu=-12257.87, \tsigma=2.0085\n",
            "Episode=50800 \tAverage reward: -1534.43 \tActor-loss=-234922530563948544.00, \tCritic-loss=0.99, \tentropy=-0.74, \tlog-prob=-111584590379876352.00, \tmu=-3786.40, \tsigma=32.3913\n",
            "Episode=51000 \tAverage reward: -1578.37 \tActor-loss=-24132204351729958912.00, \tCritic-loss=0.96, \tentropy=-2.85, \tlog-prob=-671222849138065408.00, \tmu=-12254.83, \tsigma=1.2571\n",
            "Episode=51200 \tAverage reward: -1550.31 \tActor-loss=-5441479197572726784.00, \tCritic-loss=0.85, \tentropy=-3.09, \tlog-prob=-671832390896713728.00, \tmu=-12135.80, \tsigma=0.8242\n",
            "Episode=51400 \tAverage reward: -1571.32 \tActor-loss=173.24, \tCritic-loss=0.90, \tentropy=4.31, \tlog-prob=-7.62, \tmu=7.87, \tsigma=346.1780\n",
            "Episode=51600 \tAverage reward: -1540.78 \tActor-loss=-7126406769147904.00, \tCritic-loss=0.87, \tentropy=4.26, \tlog-prob=-162481850810368.00, \tmu=-33.81, \tsigma=351.0261\n",
            "Episode=51800 \tAverage reward: -1520.83 \tActor-loss=-7439879514206765056.00, \tCritic-loss=0.93, \tentropy=-2.79, \tlog-prob=-670206625516093440.00, \tmu=-12306.07, \tsigma=1.3913\n",
            "Episode=52000 \tAverage reward: -1562.05 \tActor-loss=-61.60, \tCritic-loss=0.90, \tentropy=4.34, \tlog-prob=-7.68, \tmu=2.57, \tsigma=356.0824\n",
            "Episode=52200 \tAverage reward: -1583.29 \tActor-loss=-2602623187782991872.00, \tCritic-loss=0.83, \tentropy=3.45, \tlog-prob=-29216225120747520.00, \tmu=-736.97, \tsigma=313.6577\n",
            "Episode=52400 \tAverage reward: -1576.89 \tActor-loss=119.90, \tCritic-loss=0.89, \tentropy=4.35, \tlog-prob=-7.74, \tmu=4.52, \tsigma=364.4368\n",
            "Episode=52600 \tAverage reward: -1562.20 \tActor-loss=4132877841589075968.00, \tCritic-loss=0.88, \tentropy=-3.28, \tlog-prob=-693832244178976768.00, \tmu=-12315.19, \tsigma=0.4848\n",
            "Episode=52800 \tAverage reward: -1560.93 \tActor-loss=-21864681521767514112.00, \tCritic-loss=0.96, \tentropy=-2.60, \tlog-prob=-669434149878104064.00, \tmu=-12365.44, \tsigma=1.8274\n",
            "Episode=53000 \tAverage reward: -1528.76 \tActor-loss=-2271425683924189184.00, \tCritic-loss=0.83, \tentropy=3.44, \tlog-prob=-27204967097958400.00, \tmu=-729.51, \tsigma=325.4221\n",
            "Episode=53200 \tAverage reward: -1521.29 \tActor-loss=-172388184691310592.00, \tCritic-loss=0.99, \tentropy=-1.10, \tlog-prob=-99674019002843136.00, \tmu=-4077.57, \tsigma=22.1660\n",
            "Episode=53400 \tAverage reward: -1574.00 \tActor-loss=-1858109092560633856.00, \tCritic-loss=1.00, \tentropy=-2.37, \tlog-prob=-94584980973289472.00, \tmu=-4360.98, \tsigma=3.6214\n",
            "Episode=53600 \tAverage reward: -1543.38 \tActor-loss=-140.72, \tCritic-loss=0.89, \tentropy=4.32, \tlog-prob=-7.65, \tmu=13.08, \tsigma=366.1808\n",
            "Episode=53800 \tAverage reward: -1542.61 \tActor-loss=-1298473479502299136.00, \tCritic-loss=1.00, \tentropy=-1.98, \tlog-prob=-94313350061621248.00, \tmu=-4397.03, \tsigma=7.5856\n",
            "Episode=54000 \tAverage reward: -1556.79 \tActor-loss=-120763400192.00, \tCritic-loss=0.85, \tentropy=4.33, \tlog-prob=-3258102016.00, \tmu=-25.87, \tsigma=376.7089\n",
            "Episode=54200 \tAverage reward: -1504.84 \tActor-loss=-1218264656011853824.00, \tCritic-loss=0.99, \tentropy=-1.98, \tlog-prob=-109723297582678016.00, \tmu=-4365.76, \tsigma=9.1692\n",
            "Episode=54400 \tAverage reward: -1570.78 \tActor-loss=-6422166603734974464.00, \tCritic-loss=0.93, \tentropy=-3.06, \tlog-prob=-707005767869267968.00, \tmu=-12437.94, \tsigma=0.9188\n",
            "Episode=54600 \tAverage reward: -1559.42 \tActor-loss=-361.30, \tCritic-loss=0.91, \tentropy=4.30, \tlog-prob=-29.06, \tmu=-4.93, \tsigma=334.6458\n",
            "Episode=54800 \tAverage reward: -1587.22 \tActor-loss=10942227569440718848.00, \tCritic-loss=0.80, \tentropy=-2.72, \tlog-prob=-636666023310786560.00, \tmu=-11013.78, \tsigma=14.9979\n",
            "Episode=55000 \tAverage reward: -1581.58 \tActor-loss=-9943125542536478720.00, \tCritic-loss=0.89, \tentropy=-2.98, \tlog-prob=-668690261542436864.00, \tmu=-11687.58, \tsigma=1.0900\n",
            "Episode=55200 \tAverage reward: -1504.25 \tActor-loss=-13084355584.00, \tCritic-loss=0.95, \tentropy=4.21, \tlog-prob=-728281664.00, \tmu=-44.52, \tsigma=318.6644\n",
            "Episode=55400 \tAverage reward: -1548.73 \tActor-loss=-1018660826933886976.00, \tCritic-loss=1.00, \tentropy=-3.03, \tlog-prob=-101787950366326784.00, \tmu=-4398.88, \tsigma=0.9792\n",
            "Episode=55600 \tAverage reward: -1503.93 \tActor-loss=914030544520478720.00, \tCritic-loss=0.98, \tentropy=-4.34, \tlog-prob=-102433028684382208.00, \tmu=-4401.83, \tsigma=0.0000\n",
            "Episode=55800 \tAverage reward: -1501.91 \tActor-loss=1619504761411731456.00, \tCritic-loss=0.99, \tentropy=-2.22, \tlog-prob=-100407676726411264.00, \tmu=-4323.69, \tsigma=4.7476\n",
            "Episode=56000 \tAverage reward: -1516.73 \tActor-loss=-643.62, \tCritic-loss=0.87, \tentropy=4.28, \tlog-prob=-73.01, \tmu=-19.26, \tsigma=321.1259\n",
            "Episode=56200 \tAverage reward: -1555.63 \tActor-loss=-1656294283038162944.00, \tCritic-loss=1.00, \tentropy=-2.33, \tlog-prob=-110669015021518848.00, \tmu=-4374.39, \tsigma=5.1823\n",
            "Episode=56400 \tAverage reward: -1548.69 \tActor-loss=7792043192978767872.00, \tCritic-loss=0.85, \tentropy=-3.02, \tlog-prob=-670316439239917568.00, \tmu=-12074.08, \tsigma=0.6208\n",
            "Episode=56600 \tAverage reward: -1556.16 \tActor-loss=-5519662720644808704.00, \tCritic-loss=0.93, \tentropy=-2.90, \tlog-prob=-707344142572716032.00, \tmu=-12475.95, \tsigma=1.3291\n",
            "Episode=56800 \tAverage reward: -1532.50 \tActor-loss=7502521440420233216.00, \tCritic-loss=0.85, \tentropy=-2.25, \tlog-prob=-632752998866485248.00, \tmu=-12158.60, \tsigma=2.9839\n",
            "Episode=57000 \tAverage reward: -1540.00 \tActor-loss=8797123814359040.00, \tCritic-loss=0.99, \tentropy=-4.34, \tlog-prob=-100599747663888384.00, \tmu=-4422.94, \tsigma=0.0000\n",
            "Episode=57200 \tAverage reward: -1546.48 \tActor-loss=-17406200354042806272.00, \tCritic-loss=0.94, \tentropy=-2.47, \tlog-prob=-680891404636913664.00, \tmu=-12488.13, \tsigma=2.5631\n",
            "Episode=57400 \tAverage reward: -1548.09 \tActor-loss=-229.30, \tCritic-loss=0.97, \tentropy=4.26, \tlog-prob=-10.61, \tmu=-20.89, \tsigma=306.0225\n",
            "Episode=57600 \tAverage reward: -1495.63 \tActor-loss=-1510856382474092544.00, \tCritic-loss=0.99, \tentropy=-2.30, \tlog-prob=-113863414127788032.00, \tmu=-4380.86, \tsigma=5.4675\n",
            "Episode=57800 \tAverage reward: -1555.86 \tActor-loss=1025073797222367232.00, \tCritic-loss=0.98, \tentropy=-2.60, \tlog-prob=-101625506113257472.00, \tmu=-4257.15, \tsigma=2.2256\n",
            "Episode=58000 \tAverage reward: -1595.19 \tActor-loss=-6679375358820614144.00, \tCritic-loss=0.93, \tentropy=-2.40, \tlog-prob=-685986129203167232.00, \tmu=-12581.89, \tsigma=2.7623\n",
            "Episode=58200 \tAverage reward: -1571.49 \tActor-loss=-19288904009888301056.00, \tCritic-loss=0.93, \tentropy=-2.65, \tlog-prob=-690684685986037760.00, \tmu=-12330.65, \tsigma=2.7065\n",
            "Episode=58400 \tAverage reward: -1559.75 \tActor-loss=-11628096325777620992.00, \tCritic-loss=0.94, \tentropy=-2.54, \tlog-prob=-701036587961548800.00, \tmu=-12644.77, \tsigma=2.1918\n",
            "Episode=58600 \tAverage reward: -1524.94 \tActor-loss=-19.22, \tCritic-loss=0.95, \tentropy=4.24, \tlog-prob=-7.52, \tmu=-18.21, \tsigma=297.9073\n",
            "Episode=58800 \tAverage reward: -1565.86 \tActor-loss=4897535402149150720.00, \tCritic-loss=0.84, \tentropy=-1.82, \tlog-prob=-593026544243310592.00, \tmu=-11900.59, \tsigma=4.5878\n",
            "Episode=59000 \tAverage reward: -1543.63 \tActor-loss=7159488655961096192.00, \tCritic-loss=0.85, \tentropy=-2.17, \tlog-prob=-665795384865456128.00, \tmu=-12576.15, \tsigma=3.5545\n",
            "Episode=59200 \tAverage reward: -1554.22 \tActor-loss=-14664458159020572672.00, \tCritic-loss=0.92, \tentropy=-2.18, \tlog-prob=-679192178135662592.00, \tmu=-12743.08, \tsigma=3.2945\n",
            "Episode=59400 \tAverage reward: -1535.63 \tActor-loss=-3739.25, \tCritic-loss=1.01, \tentropy=4.26, \tlog-prob=-63.44, \tmu=-41.86, \tsigma=308.6455\n",
            "Episode=59600 \tAverage reward: -1553.35 \tActor-loss=1172238549517336576.00, \tCritic-loss=0.98, \tentropy=-4.34, \tlog-prob=-103723288399708160.00, \tmu=-4444.88, \tsigma=0.0000\n",
            "Episode=59800 \tAverage reward: -1588.63 \tActor-loss=-3048208246398517248.00, \tCritic-loss=0.87, \tentropy=-2.40, \tlog-prob=-696369161101639680.00, \tmu=-12729.33, \tsigma=2.5970\n",
            "Episode=60000 \tAverage reward: -1502.16 \tActor-loss=-1021135277852196864.00, \tCritic-loss=1.00, \tentropy=-4.34, \tlog-prob=-101248880431071232.00, \tmu=-4443.35, \tsigma=0.0000\n",
            "Episode=60200 \tAverage reward: -1530.37 \tActor-loss=8462185634503589888.00, \tCritic-loss=0.85, \tentropy=-2.60, \tlog-prob=-703852162362376192.00, \tmu=-12644.46, \tsigma=2.0730\n",
            "Episode=60400 \tAverage reward: -1504.01 \tActor-loss=2976496173889617920.00, \tCritic-loss=1.00, \tentropy=-0.94, \tlog-prob=-127166173384015872.00, \tmu=-3844.30, \tsigma=24.7414\n",
            "Episode=60600 \tAverage reward: -1539.88 \tActor-loss=1614749511060553728.00, \tCritic-loss=0.98, \tentropy=-4.11, \tlog-prob=-103461613222232064.00, \tmu=-4355.44, \tsigma=0.0001\n",
            "Episode=60800 \tAverage reward: -1542.59 \tActor-loss=-280144670720262144.00, \tCritic-loss=0.99, \tentropy=-1.50, \tlog-prob=-109322370975531008.00, \tmu=-4060.48, \tsigma=12.7755\n",
            "Episode=61000 \tAverage reward: -1567.43 \tActor-loss=4396514616528601088.00, \tCritic-loss=0.88, \tentropy=-1.94, \tlog-prob=-655085179538243584.00, \tmu=-12142.38, \tsigma=3.4606\n",
            "Episode=61200 \tAverage reward: -1536.58 \tActor-loss=-19994926814362337280.00, \tCritic-loss=0.96, \tentropy=-2.45, \tlog-prob=-719775564633735168.00, \tmu=-12909.89, \tsigma=2.6270\n",
            "Episode=61400 \tAverage reward: -1553.77 \tActor-loss=-10036715972292771840.00, \tCritic-loss=0.84, \tentropy=-2.28, \tlog-prob=-660031676193177600.00, \tmu=-12355.66, \tsigma=2.7563\n",
            "Episode=61600 \tAverage reward: -1537.47 \tActor-loss=-1076.68, \tCritic-loss=0.98, \tentropy=4.23, \tlog-prob=-304.63, \tmu=-54.92, \tsigma=309.0276\n",
            "Episode=61800 \tAverage reward: -1527.60 \tActor-loss=-10909720508165521408.00, \tCritic-loss=0.92, \tentropy=-2.61, \tlog-prob=-730874790918750208.00, \tmu=-12926.37, \tsigma=2.1304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-97c3d5693d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrunning_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mrunning_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrunning_reward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'reinforce'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-c7ebbc3d9a73>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(env, render, pause)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Don't infinite loop while learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# select an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-6286b9afa0c9>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(state, env)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncCPAuTSXnKX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDo7p8QBczwa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jogUgc5cYyf"
      },
      "source": [
        "running_reward = 10\n",
        "for i_episode in range(1000):\n",
        "    states, rewards, log_probs = rollout()\n",
        "    t = len(rewards)\n",
        "    running_reward = running_reward * 0.9 +  t * 0.1\n",
        "    ac_train_step(states, rewards, log_probs)\n",
        "    if i_episode % args.log_interval == 0:\n",
        "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
        "            i_episode, t, running_reward))\n",
        "    if running_reward > env.spec.reward_threshold:\n",
        "        print(\"Solved! Running reward is now {} and \"\n",
        "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T_v8LrRcYuU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPxtYu3DqzSl"
      },
      "source": [
        "# for var_name in actor_optim.state_dict():\n",
        "#     print(var_name, \"\\t\", actor_optim.state_dict()[var_name])\n",
        "\n",
        "d=actor_optim.state_dict()\n",
        "# d\n",
        "optimizer_state= actor_optim.state # holds all the information about the gradients, square of gradients and past gradients used in Adam \n",
        "\n",
        "te=d['state'][140575074157424]['exp_avg']\n",
        "te.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erXIKeZZrGq2"
      },
      "source": [
        "c=torch.nn.Conv1d(10,12,3)\n",
        "c=torch.nn.Conv2d(10,12,3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V95uZz_mrGt3"
      },
      "source": [
        "for i in c.parameters():\n",
        "#   print (i['weight'])\n",
        "#   print (dir(i))\n",
        "  print (i.shape)\n",
        "  print (i.numel())\n",
        "#   print (i.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5k_5eaXAT9T"
      },
      "source": [
        "# render optimal policy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uchezPDiAT9V"
      },
      "source": [
        "a = rollout(True,pause=.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42D2h_DqAT9Y"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J149hSkAT9b"
      },
      "source": [
        "z=[5,7,9,12]\n",
        "for b in [1,2,10,30]:\n",
        "  print (\"------\",b)\n",
        "  s=sum([np.exp(i*b) for i in z])\n",
        "  for i in z:\n",
        "    print (np.exp(i*b)/s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw8EBC7LaEJ-"
      },
      "source": [
        "import torch\n",
        "\n",
        "batch_size = 5\n",
        "nb_digits = 10\n",
        "# Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
        "y = torch.LongTensor(batch_size,1).random_() % nb_digits\n",
        "# One hot encoding buffer that you create out of the loop and just keep reusing\n",
        "y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
        "# In your for loop\n",
        "y_onehot.zero_()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAnWaDiWpsb9"
      },
      "source": [
        "y_onehot.scatter_(1, y, 1)\n",
        "\n",
        "print(y)\n",
        "print(y_onehot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwKVOFVZptyq"
      },
      "source": [
        "d=torch.chunk(y_onehot,10,1)\n",
        "d[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCekDr840unT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}