{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0, 1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "no_randomness = False\n",
    "load_weights = False\n",
    "semi_random = False\n",
    "\n",
    "class replay_memory(object):\n",
    "    def __init__(self, size, sd, b):\n",
    "        self.max_size = size\n",
    "        self.storage = []\n",
    "        self.cur_size = 0\n",
    "        self.batch_size = b\n",
    "        self.index = 0 \n",
    "        \n",
    "    def add(self, s,a,r,ns,d):\n",
    "        if self.cur_size < self.max_size:\n",
    "            self.storage.append([{\"s\":s, \"a\":a, \"r\":r, \"ns\":ns, \"d\":d}])\n",
    "            self.cur_size += 1\n",
    "        else:\n",
    "            self.storage.pop(0)\n",
    "            self.storage.append([{\"s\":s, \"a\":a, \"r\":r, \"ns\":ns, \"d\":d}])\n",
    "            \n",
    "    def sample(self):\n",
    "        s = []\n",
    "        a = []\n",
    "        r = []\n",
    "        ns = []\n",
    "        d = []\n",
    "        for i in range(self.batch_size):\n",
    "            indx = torch.randint(self.cur_size, size=(1,)).numpy()[0]\n",
    "            if no_randomness:\n",
    "                indx = i\n",
    "            if semi_random:\n",
    "                indx = self.index\n",
    "                if self.index < self.cur_size-1 and self.index < self.max_size-1:\n",
    "                    self.index += 1\n",
    "                else:\n",
    "                    self.index = 0 \n",
    "            s += [self.storage[indx][0][\"s\"]]\n",
    "            a += [self.storage[indx][0][\"a\"]]\n",
    "            r += [self.storage[indx][0][\"r\"]]\n",
    "            ns += [self.storage[indx][0][\"ns\"]]\n",
    "            d += [self.storage[indx][0][\"d\"]]\n",
    "            \n",
    "        return {\"s\":s, \"a\":a, \"r\":r, \"ns\":ns, \"d\":d}\n",
    "    \n",
    "\n",
    "def get_action_(env, epsilon, action):\n",
    "    if no_randomness:\n",
    "        epsilon = 0 \n",
    "    if semi_random:\n",
    "        x=np.squeeze(action_ts.detach().numpy())\n",
    "        if np.sum(x) < 10:\n",
    "            if 2*np.log(x[0]) - 5 > 3*np.log(x[1]) - 7:\n",
    "                a = 1\n",
    "            else:\n",
    "                a = 0 \n",
    "        else:\n",
    "            a = action.argmax().detach().numpy()            \n",
    "    else:\n",
    "        rnd = torch.rand((1)).numpy()[0]\n",
    "        if rnd < epsilon:\n",
    "            a = torch.randint(env.action_space.n, size=(1,)).numpy()[0]\n",
    "        else:\n",
    "            a = action.argmax().detach().to(cpu_device).numpy()\n",
    "        \n",
    "    return a\n",
    "    \n",
    "class exploration(object):\n",
    "    def __init__(self, max_,min_,num_eps):\n",
    "        self.epsilon = max_\n",
    "        self.min_eps = min_\n",
    "        self.num_eps = num_eps\n",
    "        self.eps_red = (max_ - min_)/num_eps\n",
    "    \n",
    "    def reduce(self):\n",
    "        if (self.epsilon > self.min_eps):\n",
    "            self.epsilon -= self.eps_red \n",
    "        \n",
    "        \n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, ni, no):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(ni,10)\n",
    "        self.fc2 = nn.Linear(10,10)\n",
    "        self.fc3 = nn.Linear(10,no)\n",
    "        if no_randomness and not load_weights:\n",
    "            self.fc1.weight.data.fill_(0.2)\n",
    "            self.fc2.weight.data.fill_(0.2)\n",
    "            self.fc3.weight.data.fill_(0.2)\n",
    "            self.fc1.bias.data.fill_(0.0)\n",
    "            self.fc2.bias.data.fill_(0.0)\n",
    "            self.fc3.bias.data.fill_(0.0)\n",
    "        elif load_weights:\n",
    "            d=np.loadtxt(\"fc1.weight\", delimiter=\",\")\n",
    "            self.fc1.weight.data = torch.tensor(d, dtype=torch.float32)\n",
    "            d=np.loadtxt(\"fc2.weight\", delimiter=\",\")\n",
    "            self.fc2.weight.data = torch.tensor(d, dtype=torch.float32)\n",
    "            d=np.loadtxt(\"fc3.weight\", delimiter=\",\")\n",
    "            self.fc3.weight.data = torch.tensor(d, dtype=torch.float32)\n",
    "            d=np.loadtxt(\"fc1.bias\", delimiter=\",\")\n",
    "            self.fc1.bias.data = torch.tensor(d, dtype=torch.float32)\n",
    "            d=np.loadtxt(\"fc2.bias\", delimiter=\",\")\n",
    "            self.fc2.bias.data = torch.tensor(d, dtype=torch.float32)\n",
    "            d=np.loadtxt(\"fc3.bias\", delimiter=\",\")            \n",
    "            self.fc3.bias.data = torch.tensor(d, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def test(period, avg_q, policy_net, env):\n",
    "    \n",
    "    rewards = 0\n",
    "    for i in range(100):\n",
    "        state = env.reset()    \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy_net(torch.tensor(state).float().to(device))\n",
    "            action = get_action_(env, 0, action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            rewards+=1\n",
    "            state = next_state\n",
    "    print (\"Test: episode={0:d}, Q-value={1:0.2f}, reward={2:0.2f}\".format(period, avg_q, rewards/100.))\n",
    "    return rewards/100.\n",
    "    \n",
    "def get_weight_norm(net):\n",
    "    grad_norm=0\n",
    "    for param in net.parameters():\n",
    "    #     print(param)\n",
    "        grad_norm += torch.norm(param)\n",
    "\n",
    "    return grad_norm\n",
    "\n",
    "def get_grad_norm(net):\n",
    "    grad_norm=0\n",
    "    for param in net.parameters():\n",
    "    #     print(param)\n",
    "        grad_norm += torch.norm(param.grad)\n",
    "\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def get_grad_list(net):\n",
    "    grads=np.array([])\n",
    "    for param in net.parameters():\n",
    "        grads = np.concatenate((grads, param.grad.data.view(-1).detach().numpy()))\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "model = DQN(env.observation_space.shape[0], env.action_space.n).to(device) \n",
    "policy_net = torch.nn.DataParallel(model,device_ids=[0,1],output_device=0).to(device)\n",
    "\n",
    "target_model = DQN(env.observation_space.shape[0], env.action_space.n).to(device) \n",
    "target_net = torch.nn.DataParallel(target_model,device_ids=[0,1],output_device=0).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "# optimizer = optim.SGD(policy_net.parameters(),lr=0.001)\n",
    "# torch.nn.utils.clip_grad_norm(policy_net.parameters(),max_norm=10,norm_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n",
      "Test: episode=60, Q-value=0.85, reward=9.35\n",
      "Test: episode=80, Q-value=2.86, reward=9.26\n",
      "Test: episode=100, Q-value=3.68, reward=9.40\n",
      "Test: episode=120, Q-value=6.78, reward=41.44\n",
      "Test: episode=140, Q-value=14.54, reward=198.07\n",
      "Test: episode=160, Q-value=28.42, reward=169.63\n",
      "Test: episode=180, Q-value=39.73, reward=143.91\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Training loop\n",
    "batch_size = 128\n",
    "min_replay_buffer = 1000\n",
    "max_replay_buffer = 1e6\n",
    "\n",
    "target_update = 200 \n",
    "num_episodes = 200\n",
    "show_detail = False\n",
    "log_interval = 20 # will print the details of last xx train_steps\n",
    "result = []\n",
    "\n",
    "rbm = replay_memory(max_replay_buffer, env.observation_space, batch_size)\n",
    "exp = exploration(0.9, 0.05, num_episodes)\n",
    "\n",
    "train_step = 0\n",
    "for i_episode in range(num_episodes):\n",
    "     # initialize state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Select and perform an action    \n",
    "    # keep going until get to the goal state\n",
    "    cnt = 0\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        cnt+=1\n",
    "        action_ts = policy_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).squeeze()\n",
    "        action = get_action_(env, exp.epsilon, action_ts)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        rbm.add(torch.tensor(state), torch.tensor(action), torch.tensor(reward), torch.tensor(next_state), \n",
    "               done)\n",
    "            \n",
    "        if show_detail:\n",
    "            print (i_episode,\"-\",cnt,\"-\",rbm.cur_size, state, action_ts, action, reward, next_state, done)\n",
    "        \n",
    "        if rbm.cur_size >= min_replay_buffer:\n",
    "            if rbm.cur_size == min_replay_buffer:\n",
    "                print (\"Started training\")\n",
    "            batch = rbm.sample()\n",
    "            \n",
    "            target_Q = target_net(torch.stack(batch[\"ns\"]).float().to(device\n",
    "                    )).squeeze().max(1)[0].detach()\n",
    "            target = 0.99 * target_Q*(1-torch.tensor(batch[\"d\"]).to(device).float()) + \\\n",
    "            torch.stack(batch[\"r\"]).to(device)\n",
    "            QValue = policy_net(torch.stack(batch[\"s\"]).float().to(\n",
    "                device)).squeeze().gather(1, torch.stack(batch[\"a\"]).to(device).unsqueeze(1))\n",
    "            \n",
    "            loss_model = F.mse_loss(QValue, target.unsqueeze(1))\n",
    "            if device.type == 'cuda':\n",
    "                loss = torch.nn.DataParallel(loss_model, device_ids=[0,1])\n",
    "            else:\n",
    "                loss = loss_model\n",
    "\n",
    "            if train_step%log_interval == 0:\n",
    "                if show_detail:\n",
    "                    print (i_episode, \"-\", train_step)\n",
    "                    print (np.array(batch[\"d\"],dtype=np.int))\n",
    "                    print ([i for i in torch.stack(batch[\"a\"]).numpy()])\n",
    "                tmp = [i for i in target_Q.numpy()]\n",
    "                if show_detail:\n",
    "                    print (\"target_Qvalue\", tmp)\n",
    "                tmp = [i for i in target.numpy()]\n",
    "                if show_detail:\n",
    "                    print (\"target_value\", tmp)\n",
    "                tmp = [i for i in np.squeeze(QValue.detach().numpy())]\n",
    "                if show_detail:\n",
    "                    print (\"Qvalue\", tmp)\n",
    "                            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_step+=1\n",
    "                             \n",
    "        state = next_state\n",
    "        if train_step > 1 and train_step%target_update==0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "    if train_step > 0 and i_episode%log_interval == 0:\n",
    "        try:\n",
    "            per = test(i_episode, QValue.mean().item(), policy_net, env)\n",
    "        except:\n",
    "            per = test(i_episode, 0, policy_net, env)\n",
    "            \n",
    "        result += [ per]\n",
    "        #     print (cnt)\n",
    "    exp.reduce()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
