{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/oroojlooy/RL_pytorch/blob/master/reinforce_AC_CartPole_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJqIx--gMVhT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjTnUfcUA_Bh"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade gym\n",
    "# !pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmqNcyIXAT8Y"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7Y73Kd4AT8d"
   },
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwbldOLQAT8f"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 543)')\n",
    "parser.add_argument('--render', action='store_true',\n",
    "                    help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='interval between training status logs (default: 10)')\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7tCuXp-AT8i"
   },
   "source": [
    "# create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fcX1dN16AT8j",
    "outputId": "79c4ccf0-96cd-4709-f3bf-4388cd7287a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a2f4715090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this environment has env.reset() and end.step() functions\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-y254LZFAT8p",
    "outputId": "5e2441cc-666b-4fe2-aadc-1e979aee4458"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00313956,  0.2222067 , -0.01409669, -0.31700878]), 1.0, False, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "# env.render()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7jaeYIlAT8u"
   },
   "source": [
    "# create actor network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8K7leovBfIa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnGQtBAEAT8v"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    # this class defines a policy network with two layer NN\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 30)\n",
    "        self.affine2 = nn.Linear(30, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' do the forward pass and return a probability over actions\n",
    "        Input:\n",
    "                x: state\n",
    "        returns:\n",
    "                prob: a probability distribution\n",
    "        '''\n",
    "        \n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        prob = F.softmax(action_scores, dim=1)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXQuyDXhAT8z"
   },
   "source": [
    "# Critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TlyZZTQAT8z"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    # this class defines a policy network with two layer NN\n",
    "    def __init__(self, in_d=4):\n",
    "        super(Critic, self).__init__()\n",
    "        self.affine1 = nn.Linear(in_d, 30)\n",
    "        self.affine2 = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' do the forward pass and return a probability over actions\n",
    "        Input:\n",
    "                x: state\n",
    "        returns:\n",
    "                v: value of being at x\n",
    "        '''\n",
    "        \n",
    "        x = F.relu(self.affine1(x))\n",
    "        v = self.affine2(x).squeeze()\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Vs-9MAYAT83"
   },
   "source": [
    "# Create networks and optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-__a1C3aAT84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "az8jMJnnDoau"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATtRw2qSAT8-"
   },
   "source": [
    "# rollout funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBWDaywwAT9A"
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    # this function selects stochastic actions based on the policy probabilities\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = actor(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    \n",
    "    return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfowBl_lBsUW"
   },
   "outputs": [],
   "source": [
    "# s=env.reset()\n",
    "# # select_action(s)\n",
    "# state = torch.from_numpy(s).float().unsqueeze(0)\n",
    "# probs = actor(state)\n",
    "# m = Categorical(probs)\n",
    "# action = m.sample()\n",
    "# log_prob = m.log_prob(action)\n",
    "# print(probs, m, action, log_prob)\n",
    "# # (tensor([[0.5007, 0.4993]], grad_fn=<SoftmaxBackward>), Categorical(), tensor([1]), tensor([-0.6946], grad_fn=<SqueezeBackward1>))\n",
    "# np.log(0.4993)\n",
    "# # -0.6945481614755734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_RYkLaKCPxR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZ05O7LkuTjs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TAzOnWWKAT9F"
   },
   "outputs": [],
   "source": [
    "def rollout(render=False,pause=.2):\n",
    "    states = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    next_states = []\n",
    "    mask = []\n",
    "    # play an episode\n",
    "    state = env.reset()\n",
    "    while True:  # Don't infinite loop while learning\n",
    "        # select an action\n",
    "        action, log_prob = select_action(state)\n",
    "        states.append(list(state))\n",
    "        log_probs.append(log_prob[0])\n",
    "        \n",
    "        # take the action and move to next state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(next_state)\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(pause)\n",
    "        if done:\n",
    "            mask.append(1)\n",
    "            break\n",
    "        mask.append(0)\n",
    "        state = next_state\n",
    "            \n",
    "    return states, rewards, log_probs, next_states, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSZPgcmcAT9I"
   },
   "source": [
    "# train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bq_H6jcUAT9J"
   },
   "outputs": [],
   "source": [
    "def reinforce_train_step(states,rewards,log_probs):\n",
    "    \n",
    "    R = 0\n",
    "    P = 0\n",
    "    rewards_path = []\n",
    "    log_probs_paths = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        R = rewards[i] + args.gamma * R\n",
    "        rewards_path.insert(0, R) \n",
    "        \n",
    "        P = log_probs[i] + P\n",
    "        log_probs_paths.insert(0, P) \n",
    "\n",
    "    rewards_path = torch.tensor(rewards_path)\n",
    "    rewards_path = (rewards_path - rewards_path.mean()) / (rewards_path.std() + 1e-8)\n",
    "    log_probs_paths = torch.stack(log_probs_paths)\n",
    "    \n",
    "#     print(rewards_path,log_probs_paths)\n",
    "    value = critic(torch.tensor(states, dtype=torch.float32))\n",
    "\n",
    "    # take a backward step for actor\n",
    "    # This is based on Pytorch implementation of REINFORCE \n",
    "    actor_loss = -torch.mean(((rewards_path - value.detach()) * torch.stack(log_probs)))\n",
    "    # This is based on the formual which Levine obtains for REINFORCE algorithm\n",
    "    # actor_loss = -torch.mean(((rewards_path - value.detach()) * log_probs_paths))\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step()\n",
    "\n",
    "    # take a backward step for critic\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    critic_loss = loss_fn(value,rewards_path)\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfAh0rSMaDl4"
   },
   "outputs": [],
   "source": [
    "# Actor Critic \n",
    "def ac_train_step(states, rewards, log_probs, next_states, done):\n",
    "# states, rewards, log_probs, next_states = rollout()\n",
    "# if 1 == 1:\n",
    "    P = 0\n",
    "    rewards_path = []\n",
    "    log_probs_paths = []\n",
    "    qvalue = critic(torch.tensor(states, dtype=torch.float32))\n",
    "    next_qvalue = critic(torch.tensor(next_states, dtype=torch.float32))\n",
    "    next_qvalue = next_qvalue.detach().cpu().numpy()\n",
    "    target = []\n",
    "    for i in range(len(rewards)):\n",
    "        target.append(rewards[i] + args.gamma * next_qvalue[i]*(1-done[i]))\n",
    "        \n",
    "    target = torch.tensor(target)\n",
    "\n",
    "    qvalue = (qvalue - torch.mean(qvalue))/(torch.std(qvalue) + 1e-8)\n",
    "    # take a backward step for actor\n",
    "    # This is based on the commonly used AC algorithm  \n",
    "    actor_loss = -torch.mean(qvalue.detach() * torch.stack(log_probs))\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step()\n",
    "\n",
    "    # take a backward step for critic\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    critic_loss = loss_fn(qvalue, torch.tensor(target, dtype=torch.float32))\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-asw4wcRh-G"
   },
   "outputs": [],
   "source": [
    "# Advantage Actor Critic  (single worker A2C)\n",
    "def aac_train_step(states, rewards, log_probs, next_states, done):\n",
    "# states, rewards, log_probs, next_states = rollout()\n",
    "# if 1 == 1:\n",
    "    P = 0\n",
    "    rewards_path = []\n",
    "    log_probs_paths = []\n",
    "    value = critic(torch.tensor(states, dtype=torch.float32))\n",
    "    next_value = critic(torch.tensor(next_states, dtype=torch.float32))\n",
    "    next_value = next_value.detach().cpu().numpy()\n",
    "    advantage = []\n",
    "    target = []\n",
    "    for i in range(len(rewards)):\n",
    "        target.append(rewards[i] + args.gamma * next_value[i]*(1-done[i]))\n",
    "        advantage.append(target[i] - value[i].detach().cpu().numpy())\n",
    "        \n",
    "        P = log_probs[i] + P\n",
    "        log_probs_paths.insert(0, P) \n",
    "\n",
    "    advantage = torch.tensor(advantage)\n",
    "    target = torch.tensor(target)\n",
    "    cumulative_log_probs = torch.stack(log_probs_paths)\n",
    "    \n",
    "#     print(log_probs_paths)\n",
    "\n",
    "    # take a backward step for actor\n",
    "    # This is based on Pytorch implementation of AC \n",
    "    actor_loss = -torch.mean(advantage.detach() * torch.stack(log_probs))\n",
    "    # This is based on the formual which Levine obtains for AC algorithm\n",
    "    # actor_loss = -torch.mean(advantage.detach() * cumulative_log_probs)\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step()\n",
    "\n",
    "    # take a backward step for critic\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    critic_loss = loss_fn(value, torch.tensor(target, dtype=torch.float32))\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4zw7GSYAT9M"
   },
   "source": [
    "# run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "2FGjCmdndDcj",
    "outputId": "8db919e5-5e72-4556-e073-cad06d550bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create actor and critic network\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "# create optimizers\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "Pxqb4tH4AT9N",
    "outputId": "9fd384ae-19a6-49d6-fb42-88ddad56e0ab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aforoo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast length:    44\tAverage length: 13.40\n",
      "Episode 200\tLast length:    14\tAverage length: 15.75\n",
      "Episode 400\tLast length:     9\tAverage length: 9.42\n",
      "Episode 600\tLast length:     9\tAverage length: 10.28\n",
      "Episode 800\tLast length:    11\tAverage length: 20.44\n",
      "Episode 1000\tLast length:    22\tAverage length: 32.18\n",
      "Episode 1200\tLast length:    46\tAverage length: 50.22\n",
      "Episode 1400\tLast length:    27\tAverage length: 57.92\n",
      "Episode 1600\tLast length:    72\tAverage length: 75.43\n",
      "Episode 1800\tLast length:    67\tAverage length: 93.79\n",
      "Episode 2000\tLast length:   125\tAverage length: 135.22\n",
      "Episode 2200\tLast length:    57\tAverage length: 128.27\n",
      "Episode 2400\tLast length:   200\tAverage length: 159.27\n",
      "Solved! Running reward is now 195.29744349797144 and the last episode runs to 200 time steps!\n"
     ]
    }
   ],
   "source": [
    "running_reward = 10.\n",
    "for i_episode in range(10000):\n",
    "    states, rewards, log_probs, next_states, done = rollout()\n",
    "    t = len(rewards)\n",
    "    running_reward = 0.9*running_reward + 0.1*t\n",
    "    # reinforce_train_step(states, rewards, log_probs)\n",
    "#     ac_train_step(states, rewards, log_probs, next_states, done)\n",
    "    aac_train_step(states, rewards, log_probs, next_states, done)\n",
    "    if i_episode % args.log_interval == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDo7p8QBczwa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jogUgc5cYyf"
   },
   "outputs": [],
   "source": [
    "running_reward = 10\n",
    "for i_episode in range(1000):\n",
    "    states, rewards, log_probs = rollout()\n",
    "    t = len(rewards)\n",
    "    running_reward = running_reward * 0.9 +  t * 0.1\n",
    "    ac_train_step(states, rewards, log_probs)\n",
    "    if i_episode % args.log_interval == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6T_v8LrRcYuU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPxtYu3DqzSl"
   },
   "outputs": [],
   "source": [
    "# for var_name in actor_optim.state_dict():\n",
    "#     print(var_name, \"\\t\", actor_optim.state_dict()[var_name])\n",
    "\n",
    "d=actor_optim.state_dict()\n",
    "# d\n",
    "optimizer_state= actor_optim.state # holds all the information about the gradients, square of gradients and past gradients used in Adam \n",
    "\n",
    "te=d['state'][140575074157424]['exp_avg']\n",
    "te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erXIKeZZrGq2"
   },
   "outputs": [],
   "source": [
    "c=torch.nn.Conv1d(10,12,3)\n",
    "c=torch.nn.Conv2d(10,12,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V95uZz_mrGt3"
   },
   "outputs": [],
   "source": [
    "for i in c.parameters():\n",
    "#   print (i['weight'])\n",
    "#   print (dir(i))\n",
    "  print (i.shape)\n",
    "  print (i.numel())\n",
    "#   print (i.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5k_5eaXAT9T"
   },
   "source": [
    "# render optimal policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uchezPDiAT9V"
   },
   "outputs": [],
   "source": [
    "a = rollout(True,pause=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42D2h_DqAT9Y"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1J149hSkAT9b"
   },
   "outputs": [],
   "source": [
    "z=[5,7,9,12]\n",
    "for b in [1,2,10,30]:\n",
    "  print (\"------\",b)\n",
    "  s=sum([np.exp(i*b) for i in z])\n",
    "  for i in z:\n",
    "    print (np.exp(i*b)/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vw8EBC7LaEJ-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 5\n",
    "nb_digits = 10\n",
    "# Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
    "y = torch.LongTensor(batch_size,1).random_() % nb_digits\n",
    "# One hot encoding buffer that you create out of the loop and just keep reusing\n",
    "y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
    "# In your for loop\n",
    "y_onehot.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAnWaDiWpsb9"
   },
   "outputs": [],
   "source": [
    "y_onehot.scatter_(1, y, 1)\n",
    "\n",
    "print(y)\n",
    "print(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwKVOFVZptyq"
   },
   "outputs": [],
   "source": [
    "d=torch.chunk(y_onehot,10,1)\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCekDr840unT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "reinforce-AC-CartPole-torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
