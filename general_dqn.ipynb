{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "general_dqn_multi_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oroojlooy/RL_pytorch/blob/master/general_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDVlqN7ONJ8H"
      },
      "source": [
        "import numpy as np\n",
        "import gym \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt \n",
        "import re\n",
        "import os\n",
        "import argparse\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0, 1\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJtE64G-NJ8w",
        "outputId": "04e22f6e-65f9-46c0-b3d2-979070877276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "arg_lists = []\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "\n",
        "def add_argument_group(name):\n",
        "    arg = parser.add_argument_group(name)\n",
        "    arg_lists.append(arg)\n",
        "    return arg\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true', '1')\n",
        "\n",
        "\n",
        "arg_lists = []\n",
        "res_arg = add_argument_group('prediction')\n",
        "res_arg.add_argument('--task', type=str, default='pred', help='')\n",
        "res_arg.add_argument('--lr0', type=float, default=0.001, help='')\n",
        "res_arg.add_argument('--log_interval', type=int, default=200, help='')\n",
        "res_arg.add_argument('--batch_size', type=int, default=128, help='')\n",
        "res_arg.add_argument('--nodes', type=list, default=[350, 150], help='')\n",
        "res_arg.add_argument('--output_dim', type=int, default=11, help='')\n",
        "res_arg.add_argument('--input_dim', type=int, default=484, help='')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--input_dim'], dest='input_dim', nargs=None, const=None, default=484, type=<class 'int'>, choices=None, help='', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr70x3O1NJ8-"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAdVtSf6NJ9J"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "no_randomness = False\n",
        "load_weights = False\n",
        "semi_random = False\n",
        "\n",
        "class replay_memory(object):\n",
        "    def __init__(self, size, sd, b):\n",
        "        self.max_size = size\n",
        "        self.storage = []\n",
        "        self.cur_size = 0\n",
        "        self.batch_size = b\n",
        "        self.index = 0 \n",
        "        \n",
        "    def add(self, s,a,r,ns,d):\n",
        "        if self.cur_size < self.max_size:\n",
        "            self.storage.append([{\"s\":s, \"a\":a, \"r\":r, \"ns\":ns, \"d\":d}])\n",
        "            self.cur_size += 1\n",
        "        else:\n",
        "            self.storage.pop(0)\n",
        "            self.storage.append([{\"s\":s, \"a\":a, \"r\":r, \"ns\":ns, \"d\":d}])\n",
        "            \n",
        "    def sample(self):\n",
        "        s = []\n",
        "        a = []\n",
        "        r = []\n",
        "        ns = []\n",
        "        d = []\n",
        "        for i in range(self.batch_size):\n",
        "            indx = torch.randint(self.cur_size, size=(1,)).numpy()[0]\n",
        "            if no_randomness:\n",
        "                indx = i\n",
        "            if semi_random:\n",
        "                indx = self.index\n",
        "                if self.index < self.cur_size-1 and self.index < self.max_size-1:\n",
        "                    self.index += 1\n",
        "                else:\n",
        "                    self.index = 0 \n",
        "            s += [self.storage[indx][0][\"s\"]]\n",
        "            a += [self.storage[indx][0][\"a\"]]\n",
        "            r += [self.storage[indx][0][\"r\"]]\n",
        "            ns += [self.storage[indx][0][\"ns\"]]\n",
        "            d += [self.storage[indx][0][\"d\"]]\n",
        "            \n",
        "        return {\"s\":s, \"a\":a, \"r\":r, \"ns\":ns, \"d\":d}\n",
        "    \n",
        "\n",
        "def get_action_(env, epsilon, action):\n",
        "    if no_randomness:\n",
        "        epsilon = 0 \n",
        "    if semi_random:\n",
        "        x=np.squeeze(action_ts.detach().numpy())\n",
        "        if np.sum(x) < 10:\n",
        "            if 2*np.log(x[0]) - 5 > 3*np.log(x[1]) - 7:\n",
        "                a = 1\n",
        "            else:\n",
        "                a = 0 \n",
        "        else:\n",
        "            a = action.argmax().detach().numpy()            \n",
        "    else:\n",
        "        rnd = torch.rand((1)).numpy()[0]\n",
        "        if rnd < epsilon:\n",
        "            a = torch.randint(env.action_space.n, size=(1,)).numpy()[0]\n",
        "        else:\n",
        "            a = action.argmax().detach().to(cpu_device).numpy()\n",
        "        \n",
        "    return a\n",
        "    \n",
        "class linear_exploration(object):\n",
        "    def __init__(self, max_,min_,num_eps):\n",
        "        self.epsilon = max_\n",
        "        self.min_eps = min_\n",
        "        self.num_eps = num_eps\n",
        "        self.eps_red = (max_ - min_)/num_eps\n",
        "    \n",
        "    def reduce(self):\n",
        "        if (self.epsilon > self.min_eps):\n",
        "            self.epsilon -= self.eps_red \n",
        "\n",
        "class multiplicative_exploration(object):\n",
        "    def __init__(self, max_, min_, reduction):\n",
        "        self.epsilon = max_\n",
        "        self.min_eps = min_\n",
        "        self.reduction = reduction\n",
        "    \n",
        "    def reduce(self):\n",
        "        if (self.epsilon > self.min_eps):\n",
        "            self.epsilon = self.epsilon * self.reduction\n",
        "\n",
        "        \n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, id, od, layers_d, activations=[], dropouts={}, \n",
        "                 batch_norm={}, fixed_weight=None, normal_weight_mu=None, \n",
        "                 normal_weight_std=None, uniform_weight_l=None, \n",
        "                 uniform_weight_u=None):\n",
        "        super(DQN, self).__init__()\n",
        "        self.id = id\n",
        "        self.od = od\n",
        "        self.layers_d = layers_d\n",
        "        self.dropouts = dropouts\n",
        "        self.batch_norm = batch_norm\n",
        "        if len(activations) == 0:\n",
        "            self.activations = [F.relu for _ in range(len(layers_d))] + [None]\n",
        "        else:\n",
        "            self.activations = activations\n",
        "\n",
        "        for c in range(len(layers_d) + 1):\n",
        "            # print('layer {:1} has {:2} nodes'.format(c, l) )\n",
        "            if c == 0:\n",
        "                module = nn.Linear(id, layers_d[c])\n",
        "                if c in self.batch_norm:\n",
        "                    self.batch_norm[c] = nn.BatchNorm1d(layers_d[c])\n",
        "            elif c == len(layers_d):\n",
        "                module = nn.Linear(layers_d[c - 1], od)\n",
        "            else:\n",
        "                module = nn.Linear(layers_d[c - 1], layers_d[c])\n",
        "                if c in self.batch_norm:\n",
        "                    self.batch_norm[c] = nn.BatchNorm1d(layers_d[c])\n",
        "            self.add_module('Layer_' + str(c), module)\n",
        "\n",
        "            if c in self.dropouts:\n",
        "                self.dropouts[c] = nn.Dropout(self.dropouts[c])\n",
        "                # print('dropout is added to layer {:1} '.format(c))\n",
        "\n",
        "        if fixed_weight is not None:\n",
        "            for p in self.parameters():\n",
        "                torch.nn.init.constant_(p, fixed_weight)\n",
        "\n",
        "        if normal_weight_std is not None:\n",
        "            for p in self.parameters():\n",
        "                torch.nn.init.normal_(p, normal_weight_mu, normal_weight_std)\n",
        "\n",
        "        if uniform_weight_l is not None:\n",
        "            for p in self.parameters():\n",
        "                torch.nn.init.uniform_(p, uniform_weight_l, uniform_weight_u)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = x\n",
        "        c = 0\n",
        "        for c, (name, module) in enumerate(self.named_children()):\n",
        "            if self.activations[c] is None:\n",
        "                out = module(out)\n",
        "            else:\n",
        "                out = self.activations[c](module(out))\n",
        "            if c in self.dropouts:\n",
        "                out = self.dropouts[c](out)\n",
        "            if c in self.batch_norm:\n",
        "                if self.batch_norm[c]:\n",
        "                    out = self.batch_norm[c](out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def get_norm(self):\n",
        "        list_ = []\n",
        "        for c, i in self.named_parameters():\n",
        "            list_ += [i.grad.reshape(-1,)]\n",
        "        norm = torch.norm(torch.cat(list_))\n",
        "        return norm.item()\n",
        "  \n",
        "    \n",
        "def test(period, avg_q, policy_net, env):\n",
        "    \n",
        "    rewards = 0\n",
        "    for i in range(100):\n",
        "        state = env.reset()    \n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy_net(torch.tensor(state).float().to(device))\n",
        "            action = get_action_(env, 0, action)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            rewards += reward\n",
        "            state = next_state\n",
        "    print (\"Test: episode={0:d}, Q-value={1:0.2f}, reward={2:0.2f}\".format(period, avg_q, rewards/100.))\n",
        "    return rewards/100.\n",
        "    \n",
        "def get_weight_norm(net):\n",
        "    grad_norm=0\n",
        "    for param in net.parameters():\n",
        "    #     print(param)\n",
        "        grad_norm += torch.norm(param)\n",
        "\n",
        "    return grad_norm\n",
        "\n",
        "def get_grad_norm(net):\n",
        "    grad_norm=0\n",
        "    for param in net.parameters():\n",
        "    #     print(param)\n",
        "        grad_norm += torch.norm(param.grad)\n",
        "\n",
        "    return grad_norm\n",
        "\n",
        "\n",
        "def get_grad_list(net):\n",
        "    grads=np.array([])\n",
        "    for param in net.parameters():\n",
        "        grads = np.concatenate((grads, param.grad.data.view(-1).detach().numpy()))\n",
        "    return grads"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZIz5rgtNJ9y"
      },
      "source": [
        "config, unparsed = parser.parse_known_args()\n",
        "torch.manual_seed(298)\n",
        "env_id = \"MountainCar-v0\"\n",
        "env = gym.make(env_id)\n",
        "config.nodes = [128, 128]\n",
        "config.batch_size = 128\n",
        "config.min_replay_buffer = 1000\n",
        "config.max_replay_buffer = 1e6\n",
        "\n",
        "config.target_update = 200 \n",
        "config.num_episodes = 1500\n",
        "config.show_detail = False\n",
        "config.log_interval = 100 # will print the details of last xx train_steps"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmU-rLEQLokE"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxD8lqUONJ9Y"
      },
      "source": [
        "\n",
        "policy = DQN(env.observation_space.shape[0], env.action_space.n, config.nodes, \n",
        "             uniform_weight_l=-1, uniform_weight_u=1).to(device)\n",
        "            #  fixed_weight=0.01).to(device) \n",
        "            #  normal_weight_mu=0.0, normal_weight_std=1.0).to(device) \n",
        "            #  =0.0, normal_weight_std=1.0).to(device)              \n",
        "\n",
        "target_policy = DQN(env.observation_space.shape[0], env.action_space.n,                     \n",
        "                    config.nodes, uniform_weight_l=-1, uniform_weight_u=1).to(\n",
        "                        device)\n",
        "                    # config.nodes, fixed_weight=0.01).to(device) \n",
        "                    # config.nodes, normal_weight_mu=0.0, normal_weight_std=1.0\n",
        "                    # ).to(device) \n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters())\n",
        "# optimizer = optim.SGD(policy_net.parameters(),lr=0.001)\n",
        "# torch.nn.utils.clip_grad_norm(policy_net.parameters(),max_norm=10,norm_type=2)\n",
        "\n",
        "rbm = replay_memory(config.max_replay_buffer, env.observation_space, config.batch_size)\n",
        "exp = linear_exploration(0.9, 0.05, config.num_episodes)\n",
        "# exp = multiplicative_exploration(0.9, 0.05, 0.98)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7StHo6fkNJ9l"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0e5WVWXo3Y5"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wDQ0prWSNJ99",
        "outputId": "c54dfd42-d3be-4c04-a049-554832ac7cdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "######################################################################\n",
        "# Training loop\n",
        "\n",
        "result = []\n",
        "\n",
        "train_step = 0\n",
        "for i_episode in range(config.num_episodes):\n",
        "     # initialize state\n",
        "    state = env.reset()\n",
        "\n",
        "    # Select and perform an action    \n",
        "    # keep going until get to the goal state\n",
        "    cnt = 0\n",
        "    done = False\n",
        "    rewards = []\n",
        "    while not done:\n",
        "        cnt+=1\n",
        "        action_ts = policy_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).squeeze()\n",
        "        action = get_action_(env, exp.epsilon, action_ts)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        rbm.add(torch.tensor(state), torch.tensor(action), torch.tensor(reward), torch.tensor(next_state), \n",
        "               done)\n",
        "            \n",
        "        if config.show_detail:\n",
        "            print (i_episode,\"-\",cnt,\"-\",rbm.cur_size, state, action_ts, action, reward, next_state, done)\n",
        "        \n",
        "        if rbm.cur_size >= config.min_replay_buffer:\n",
        "            if rbm.cur_size == config.min_replay_buffer:\n",
        "                print (\"Started training\")\n",
        "            batch = rbm.sample()\n",
        "            \n",
        "            target_Q = target_net(torch.stack(batch[\"ns\"]).float().to(device\n",
        "                    )).squeeze().max(1)[0].detach()\n",
        "            target = 0.99 * target_Q*(1-torch.tensor(batch[\"d\"]).to(device).float()) + \\\n",
        "            torch.stack(batch[\"r\"]).to(device)\n",
        "            QValue = policy_net(torch.stack(batch[\"s\"]).float().to(\n",
        "                device)).squeeze().gather(1, torch.stack(batch[\"a\"]).to(device).unsqueeze(1))\n",
        "            \n",
        "            loss = F.mse_loss(QValue, target.unsqueeze(1))\n",
        "\n",
        "            if train_step % config.log_interval == 0:\n",
        "                if config.show_detail:\n",
        "                    print (i_episode, \"-\", train_step)\n",
        "                    print (np.array(batch[\"d\"],dtype=np.int))\n",
        "                    print ([i for i in torch.stack(batch[\"a\"]).numpy()])\n",
        "                tmp = [i for i in target_Q.to(cpu_device).numpy()]\n",
        "                if config.show_detail:\n",
        "                    print (\"target_Qvalue\", tmp)\n",
        "                tmp = [i for i in target.to(cpu_device).numpy()]\n",
        "                if config.show_detail:\n",
        "                    print (\"target_value\", tmp)\n",
        "                tmp = [i for i in np.squeeze(QValue.to(cpu_device).detach().numpy())]\n",
        "                if config.show_detail:\n",
        "                    print (\"Qvalue\", tmp)\n",
        "                            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_step+=1\n",
        "                             \n",
        "        state = next_state\n",
        "        if train_step > 1 and train_step % config.target_update==0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "            \n",
        "    if train_step > 0 and i_episode % config.log_interval == 0:\n",
        "        try:\n",
        "            per = test(i_episode, QValue.mean().item(), policy_net, env)\n",
        "        except:\n",
        "            per = test(i_episode, 0, policy_net, env)\n",
        "            \n",
        "        if env_id == \"MountainCar-v0\":\n",
        "          if per > -110:\n",
        "            print(\"It is solved!\")\n",
        "            break\n",
        "        result += [ per]\n",
        "        #     print (cnt)\n",
        "    exp.reduce()\n",
        "        "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test: episode=0, Q-value=5.62, reward=-200.00\n",
            "Test: episode=100, Q-value=-36.95, reward=-200.00\n",
            "Test: episode=200, Q-value=-23.53, reward=-200.00\n",
            "Test: episode=300, Q-value=-29.31, reward=-184.71\n",
            "Test: episode=400, Q-value=-31.61, reward=-200.00\n",
            "Test: episode=500, Q-value=-32.90, reward=-131.25\n",
            "Test: episode=600, Q-value=-31.24, reward=-155.13\n",
            "Test: episode=700, Q-value=-39.25, reward=-112.66\n",
            "Test: episode=800, Q-value=-36.28, reward=-133.20\n",
            "Test: episode=900, Q-value=-39.09, reward=-154.36\n",
            "Test: episode=1000, Q-value=-37.95, reward=-124.95\n",
            "Test: episode=1100, Q-value=-41.23, reward=-149.84\n",
            "Test: episode=1200, Q-value=-44.28, reward=-155.86\n",
            "Test: episode=1300, Q-value=-41.00, reward=-122.02\n",
            "Test: episode=1400, Q-value=-42.60, reward=-124.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1V_carZNJ_b"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnf_SizVNJ_h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}